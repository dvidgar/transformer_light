{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhSmR2sClF2q"
   },
   "source": [
    "# Image classification with Vision Transformer\n",
    "\n",
    "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
    "**Date created:** 2021/01/18<br>\n",
    "**Last modified:** 2021/01/18<br>\n",
    "**Description:** Implementing the Vision Transformer (ViT) model for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdRUkrzIlF2s"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n",
    "model by Alexey Dosovitskiy et al. for image classification,\n",
    "and demonstrates it on the CIFAR-100 dataset.\n",
    "The ViT model applies the Transformer architecture with self-attention to sequences of\n",
    "image patches, without using convolution layers.\n",
    "\n",
    "This example requires TensorFlow 2.4 or higher, as well as\n",
    "[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n",
    "which can be installed using the following command:\n",
    "\n",
    "```python\n",
    "pip install -U tensorflow-addons\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4t7PrBOglF2u"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "saU-TVKRlF2v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pa7__gaulF2v"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxrdXFqMlF2w",
    "outputId": "efaf4923-61a0-4c4d-da86-ce6602be22b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1KSPGeRlF2x"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hGLdPi98lF2y"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 70\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6nS03VTlF2y"
   },
   "source": [
    "## Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Cxqw5LRclF2z"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHlow8ValF20"
   },
   "source": [
    "## Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "U0DOttK1lF20"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIYt9MLLlF21"
   },
   "source": [
    "## Implement patch creation as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qFsGYVg2lF22"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRhY_wWhlF22"
   },
   "source": [
    "Let's display patches for a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "gjZ5_CtTlF22",
    "outputId": "47af9281-7327-44cc-991d-20c6445d2383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 72 X 72\n",
      "Patch size: 6 X 6\n",
      "Patches per image: 144\n",
      "Elements per patch: 108\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD7CAYAAABOrvnfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztfXfUbVtV39zttK/3e1/jFeoDRHxgFKyAWNFYiaISDCFkJCrGGBMdZoxYhl1UoqJB1BEVY0mMSokSEZWMIEaNJcJDynu8+275+vedvlv+WGX+1t1r3e+SMU4y4p6/P+5Zd3/7zLP22mXOPctvRnVdk0AgaAfi/9cTEAgE//cgN7xA0CLIDS8QtAhywwsELYLc8AJBiyA3vEDQIsgNLxC0CHLDCwQtQroIoZ/6hXfbbJ40TYiIaGW5Z/++NOjacZmXRERU5IXd9qs/896IiOhL//5TrZxIf3ayxO436PH000yNc8gj+nev/R/ma/SK1zzH/iWnrvNJRJRkayyru05ERN3BNsv6lu+3sr72R77Pyur0N/T3B/z9tG/H5olqv0xE3/FVX2T/+y0/8yYrq8inRERUVTO7b1Xl/MV6rj5L/vsPfd23WFnf8EPfxrImB+pzvM/fL87tMKqGSlQxt9t+8rXv4fX6umdbWWVVqWMB9YDjbqr+00l54498D6/9q/8Zr/1UT3044mM4OZ/Y8Xii5lPCeXz3Wx+NiIie+1l8XZW5+eQdpxM+lulYyc/npd32+PuHdk679w/sFyu9S13DWarwjOnjgr8fXjm1/9m5Y9XKSmolbKnP1+nGWmbHa6vqmltZ4Wvk13/rA1bWl3/Zg1bW2Ugdz9mQj+tsyGt1qtctL/gYr3xwghNvQDS8QNAiyA0vELQICzHpy7Ky4yhSFkZR8LYcxkVeOZ+IOWyLPKM0YXOuJrVvSf7agLJks6eOSj033hYRv1JEtR6jOQ2oS95e6XES8/cpLmGsnqlx5Le04pi3J0ms55XC3+EYtWld1SAfEMVsRtaU6H15G8E4qszfQ8982FfPPYp4LnGE6xzp+fklVXA9lIX6Hl4PJY71vqXnNBYFbzRy8LsVfKluvAy6iCM+bjPE+eOyGAlRoOykA3dRqoX1uiyg1+W17OrtGVv5DpKUfyTVcnHfbpePZ1AoWXlx+/UwouEFghZhIRp+OmNtl+qnbhSzVizAIWKcKuhcMTg+R62rvtODp10+Zzndjno814lf+82m7OyIMzWnJIOnac2Ck0qNo6LjlVXOTllWpJ7eYGxQDdojTpWMJPU/0rMEHI/6cGo4LXXEjsVaWxz5HLQ2IErZcVjHyimU1+wcqtFiKY0VE/LxgEMzrvRn2dhGRBRVWttWfk0zm/J5HA3VHEYjnstkwn+fTLXF55E1GvN+pbb+yjlqeBhXxirx67QU1t3ugr8JQ2OExYGlWlvm3+hrDby6zPLX1/g6Wl5S10Gn65/XoA/HoK1WvLg6PT73K8vqHFVVwLTyQDS8QNAiyA0vELQICzHp52CeF9qkr+DZMi9w36rxHYOzIZr06vtzsIxLkFNoEynp+E36EmKVWaZ+s4vOFnSW6NeCiDj+iUhKfj2gfKT2TdlsS2owh7XJn8V+k76TonmuxzHakzw0Mfko4ABMMjbfSZv0ZcTbqprN6FovXhxwalHEeRPGIZklaNKDE1SvbVX6174As9uY9/MZ71vmPIfaOBM9XruqiOE7SmaOjl34SqQXLk38x9dNwTFq1qBGWTw26QVp7H9l2Vjhc7i6rMarK83YOxFRf6B+NwrI6vf4d2u9T5TxMSyh49V4Fj8KDhvR8AJBiyA3vEDQIizEpCew7AxnXgkx7xq8oSa2WvpMOIg3G7MqipvxUyKiVAcrV5bXvVPa277Xjjc3t51PIqJOh01fkxpbkt9L/8S77+b5633QQ47mfaxj41HAxdvPcHvTHRyBSWpi5in5vbJL/SX+Xq5SSyMws4uEzcHSrGM18srqpeilV3PIIM6fJmh66ji836KnlcEG/0e/lqwu8aWXdXjeJqrh8/g/95nPsuPz83Pnk4gon/MrWFkUek7w3gfYWufzHencWjTjY7CTTUy91/Hrx7su8/xXtXm/1Ofj6zsp4DovJeBZ76R8vkuTSwHXA+ZzmJh/KHrgg2h4gaBFWLiGN8qorCGGCg9dEzotPQ+8GlOftEaKICbpaHgd515Z2fJOCTX83Xc+gYiI7rrzHrut12UnlZE1902KiJ549112PNHFP3nF2i8Hx0qptwdE0QA0fG2evzHG8WP4u5KVBp7oS32wMrSGi+B356AdrPuuCDi1MtDwWttlsPY4rk0OQ+q/nFb6rOFTfXH0+it229bWnh0vL6vtPj/Uc5/5sXZ84+C6+ty/YbdNJmytTKdjIiLKZ+BgBWytQX6CLiBKCB11PF4aqONbHvgdr3ddWrbjtVV1jrod0Mqggo3FO839lkcHfqLSFlUNjt0MrLSeXu/ko1DxouEFghZBbniBoEWIpBGFQNAeiIYXCFoEueEFghZhIV76Zz1/D1zp+hMfLRF6pt1PIqK/eOe1iIjo4154J1NJ6Tj76oA90Rtrq3a8u7VLRET33HWf3fZNr/kR+0Nv+pXXWlk72iu8Dd7hDIqOE+0lx1jpMx78FCvrPX/+ditrpr3hWAGIWcITnUo6gYqxv/eZX2B3/sW3/UYjjxYr67Iux/RrvW6TKdNDfeWLPtPK+qnf/M9W1ujsWH2eHtt9p6MDO54NFfVVDbRXP/Bdb7KyvvlbvgDWXg1XltgbvQzjTrbkfBIRffUrvtPKeuMbgHpLhwe6PT6Pa6vsxR+YSANcIy/+7K+KiIj+y1t/zso5PjkiIqKj0yO732h0xuOhOq7JeGi3fe/3/rIV+pqv/Rwraz5V3v1ixl7+Ys7efeM5zyCi8ro3/qX9zze++ulWVtbVlZiYMQ1OdMOSVUDeyfe99q+ZpuzrmdbNXEeQhUwJ3EipFoyx+df9+F/e0mUvGl4gaBEWE4cnZF4xgxDji/uJ6EDNclc/Mpch1ry9sWPHd11WMfX7nvBk7+88+YFn2PGSjgEPIBYcYwYfmYIff/B8F3631HFltAbmUKhzeqq0xmkgo221g9aOGneAIaU34PyAKFbrMUz89fBrPSDPnGrNDWc4BlaeKFKEmbX+vBldyKQbdFRMfguOe2f7DjteWd4kIqJl/XkzPuYZz4H/6aw9yEbsdXnenY7mD/DElp/2pKfY8XiqNPdoyut6dnYKY2XZDGEb4sGnPtWOh3rf85MD2MaWQ63JQ6sAAxKG1Me6MqzCBAi4tuvYpox6ZQ1nvF2neDhMPzUw/JDeHmIa8kE0vEDQIiyG08555Pg0O2Qh6bGvitHNKjLv8KyVd7cu2/Edl5SG39thzYPY3mTtlCZKY2WQL+5FgMSs32Urw1gBJWj4bsqP/GqmmXg85b9ERGsDyOnWT/2sw/NyNLy2eLJAeewmUB+nhZKRQD1xApTOtaZ0rsg/rwEwsizrOa7Du/bWxiU7Xl1V2Y3BLEc8J3ruSQz55cj6o3n9Yo/Jt7rCdRK9vprTcsHXw/ISj9dXFe34eMTv8IgH7mdr4Vxr+KMD9gkdH/I5Pj9Xfz87P/HKmoDiP5+qdS3AOsRsyUhnzSWBdMlJydtNWTGWF+fgC8o1U5CvlDgE0fACQYsgN7xA0CIsxKQvauyWYsgEkfYYOYDDDIEZ7NfTxRyrUP56CUzFy3vKpN9Y85uVPXBoGTYU/MnaiZ3cNLebEIFTMrZ8xv7XmIEpu13yy9pYBsehfoXBECGa98az2QmwuGxDyWdWqVeBtIQS3zk4hEaapSbxm4MDKBRZWVHm7eoKd+dZXWUH3dJAnZNBj48F0e1yuM6sLbL2OOa7KbX1yEFKbUPlnaTY0Ygv515HzXltxe9IvOcedu5OxsrBubLMxzfoc9jxsSuPEBHR8dBfiAN8nHQ0MiY9MCwB82pmnJZeSW5IN9emfI4kn9CFZjxU4VnTvel2IBpeIGgR5IYXCFqEhZj0KXogrekGLC5gqmunLCUe87kL5uzyQJlYG2tsou1sX4KxyppbWWZPKyKD7LXal96H8MzZ+TN2ZdFCIkJzE19F1GcceLauDNjcTbRJn0B0IkaSS0OmGOC4X1vm9YoLJTeteD3qOZvc5UytZ1n4ZaH5vqxfo5aW0OTlca+nfitDEk1ABmZ37QkaO6Schk/Se26Q7UhduimY+WmHX18MnX8UOIdbEOGZ6+NLE16LFPIEhtrzvn/CmXzO/KF56LxWnvwZRG2Qzcl41OvAxTeHSIoheM2nbLKPx/z+MNTmfRGIAPkgGl4gaBEWouE3NlhrmRa8TiteGBu23tjzSF9ZZsfJjs6V393etdu2gJPOaKQs8/PQVcCRFjUGoWyBQHYgNS0Yp9MaOqH0CvviykREXdBKJica+e+wn5sZZQGGkx5aAzomjfsmwCto2m7PA4wwG5vMBtTXzrh+jx2mGWi1JDKa8WLmFV8nGMe4M2vg+a7TD05/Viiu9o1Dc+K1Mo6/5RW+nmro73c6VtmIw5mftnxrj2syzkoV90+mHP+PE7Bo9byqgKNtDi2v5xO1zxw0/HzOB5lrtqKivH29LRpeIGgR5IYXCFoEYbwRCFoE0fACQYsgN7xA0CIsxEv/eV/5IJfB65Ebi4SdNWF7DTW/v/MbD0dERF/zqhfajffdfT8RET35iQ/a/Z76NO5Ecsdl7gZjsLP3JOui3b/+N+y3jcwnesObHWDQS7+xfa/9z/H+o+A6b3qV3bckE3fluOzmHss6uv6hZlNyrKWG79nYLfzY1s5T7P8ODt5rZZle6SUQ4g+Hh3Z8fqYYbwx/OxHR8z7pq62sP3jHG60sU124scZ5D2urHC0xMesUUoI3Lt1vZZ34jpECa+9xqq/tqPU63f8wyNHXjdPRCH7F7sk6bXPvASv98PoHG+cQz1GRs7f8Qx95PxERfVh/EhF9+Uu/0cr69u/7Givr4Q//JRERnZ5zPT1eELVprgoO/7f82getrM96yf3MpjTVjVYnPK/xiNNsRzqnN4dz/KEPHArjjUAgUFiIhsd4aaUfuzVmHgFrR2Tqfz2NODBDa3lJZ3sNOMOrA3+Pddy0+ijoP1yH5e07L2soDvKVzLuP2Oqmz5v2dRhnzD6e1j3ql52PhqyKY+qRPrUx9IPr9ZZhZzXL/oD58RCbW1yYlOi1xeIYzPZLEpM/EJrYBZlgt904BS6SyLOuaLGR0dqhOaFVYa5XrFvnv/d0Ic0q8AEgNlY5P2Fb1+F34RxiG23bf6/vv/X2NtlymutipxyKns7OgWuvq8bzQBcbH0TDCwQtwmI0PDXfh+sSNTxryLhUU4jKJk9bloCGHxgNz7nhnQz53nSX1ttQ8EaxO1lsHm0f1Pm1j9ssZC1Ujd9CRMQavrZawXFywL7aWgpNrEYNrzLh4og1cbfHGZAdzRpbB7qYooY3ijuJOCswjVmu1eyBxY+ippVysT3lM51QkzU1vFt2nQTF6J3hP4ZnDku4edzVGn4lqOF5+47W8D3opYhZdVVpmGb9bEt7G6zh81KtcV7xund7mMGncvtnc38GoA+i4QWCFkFueIGgRViMSQ+Os1rH4KqCTZxizuOedrz1wdw0WFliZ8j62nZjG5JQRiY8FqDDjmo09/T8nIqXpkkeBRxttc+pBiZcREgHXTb+7qA6h/+UWj6aqeiUagxuAjvgbAkvFIGgyVtbpiG/JGy6QLrwKCafSU1UR2a9Qg4yzyuQJ1AX3MFuQTk+Z2js+XtgSnAOa08YFi3+TBclDXr8ConYWGFn6HxTmfcbQDWOpdFJrMzzrMPOZ8THPPhMO55pk958EhGdnnMY9fhUXTvzuTjtBAKBB3LDCwQtwkJM+gRM+lLHICuIFeZggizp7KxBv8lUswoEhBvr2qRfRpMeat9txpv/GebfHjLZbx07d5MGzBg9pRDbtt1KAp7UijujcCZdswZe/0F/+jvPOL9r94FTHCIS9SCOITpg5gOvJb5swhBq59jN70LHHV/YwRvVwHU38wuZ9OY1I8RahK9lsf5GM7pExCZ9v+vnWkCTPt5U12wFPQK6HX4V6HXVvoNl9sYjnv3MZ9vxuFC/Oy75HI7GHNU5H6moTFEI441AIPBgIRq+g1L1AzgGLRAB+4zpCru01KQ4XgYK52XNftPrs6POabGmtU/I0RZ5HG3oEKudeHeht4XiyiCrNvtCPL1ixwqV6ilcl/4ebmUOPGnawRZBdhwhi42NLV/MtWeZY6JmToCar/mOH846Rs38ANex6Mq8GaY3m5Kb2lFgb/MljxzIcLRzQa0PDsoL4v0+De/k94MuzPT5MFTpN2MZOuGSuWZL7J3HGr6vLdklYNdB7ACj07k22FIw3DrQ9WiwpNYD6yUugmh4gaBFkBteIGgRhPFGIGgRRMMLBC2C3PACQYuwEC/9K1/FjDfTmfKGHh6z5/r0nL2td+zeR0REl/futdte96NvioiI3viGH7ByHnr2c4mIaG+HvZuY/pnYBou8bX33qcy6cuO9zWROp4oLY8xmzH/f2H0us9TsvwcC5epY6pK97XXB43I21J+cQnvH07/eyrr23tczs0yqPLAxVgFiM8nEeH7ZA7yx+0Jm4rnx+8Dq09yXPKw+eIzr2x/H6wXHWNuoBXjEMVXYk/K7ufNpwBD0h7D2+pLDVGeXL6gxr43dT4iIiI5vvAvk6N9Hzz2mEdt2jbx+a7vPsT90euMvIGzU1b/M369AF+azkfqcj+y2u+7/eCvr4T/5j1bW6PhRJQu4CTrABNTrKy/+0go0Qn3wJbxWH363lXWkGW2OhnyMBVad1s0uNp/8/M8QxhuBQKAgN7xA0CIsploubo7RmozhP1mmptDrNyuRutBXu9NVgtIMLDEgRDCJIFEo7TTCpI1mN8naqXC7IPHGSUAxpiUnl9QVJ9mUhXqVMWbhzShzzqpIIr0GKT6HsdrNmOf+BpARkF34UokjL8lHKC3TV/3Ha1hHvgotvzWJVW7QZCzwPT2vujmvGpkfrcwcd2CJlqg0lGqNyVPme552ZESU6MaVlPir5bodrvQstcleF3zeEqDLMoSfSaAhaJzw9+JYzSuGtU4hESvVN1fkacQagmh4gaBFWIiGn2DzO03v4+T3I9mgdrY5LaY1zBNOfUU7x6BAxNUy5uns1/BuLbUPkHZqnSGh4hqP8wqdWKidrPIIPFtrPAVagwOVVOSMMz2/kCzff2AuUfPvUUDDY02/WTtferIrOETjhdrUbLu1hq88NGJlBTmmdVPDxzFaf0Zm6BJv0n/7C5WYJixK/euedTgFvNvbUtKLIewBxxLf+hxWYIWlqZp7v488Btgy25oxXlk+iIYXCFoEueEFghZhISb9aMImnKnVzQs0mZudRny+lShGs7FZV+442qxZ5neGOLXc1lzzxX8xrhlIO3YcSnlDvktnZaq/Qs6jplMOzXhyxmlj1o4srIzzOKLIx8Mf4IyPnFegZl5C5JjEty6Xi2rfq4C/84x9naqaJn1VYr2/fsXz5gMQxfrVLgocX405GHXzfCNVWmScdpH/dkGTvu6r67PI+dWyKqF6MjJ8+f7rAU+RNekHWAWJJr2hMfOK8kI0vEDQIsgNLxC0CAsx6Q+PIQ6tUwFncyDjr/g5M50q02w64VREg/mczTrTjLKuQjRNhuIqcEh1t7GvCx8hgh+u5awpvEp+1aigEaEh/jD5BjcDGzAmmTZDEzBxgWqqtlRTIZ5XXnezNi6Lr+e4g625PC2uAlSzF5FNePuI4SuU89Zh4vDN7yCRhonT42tdXEMewoVmro8YBEk/PFGIgNAImFgSTZJRR5AzUGD6r84XCVym2K4r1TH5PnH8H730FPteTW8N0fACQYuwGA1/xNraPL2B1cppcGg0/GTSpICaz0Brlk3nV1SDtrfx90CmXe2nJ2Kghm86qVxZqJJ0Vh48xcsCsuf0MzULZFah5k/1uIachAKcTpWeVxzsp4Vr2Gyf5FJjmXUMHaPP0RYksVI/Ffirv3VWgHrLanhPph1oeNuktG5+V03GUHxdQKVFmG+Ba4G/b5xjflkxZL/F+jpLYtDwMTobNRVbEpgXnK5Ma3hsuYYa3u98vjVEwwsELYLc8AJBiyAUVwJBiyAaXiBoEeSGFwhahIV46e97cOuW7wkZeDXvf8K9RET0wL332m0/9rpfjoiIfuanv9vK+fiHHiIiot2dHbtfDIX3sfZGYz38+qWnMWXTtffBnJpeZ4zn1rZ2nr2rm3vPY1nX/6v9YlUq6qp8cmj3LaYndpzqeaXgLd962teyrA/+nJWVdFWTggpi8zmkF1d63jF4sLd3vpipt67/ipUV61MbO80ZMILR9Dyv7L7Y/ufsxtuaQfdge6kmLdXqHtM2nd749SY1lcMpAOdBh3OQymnnzpdFRETXHvl5u2NZqb+b/Awiom53CcYq3TWOuXHD6vYn85z2/4gpvGwEh9cKU7Q534Ovt/Wd+62sw6t/xfMqRvqTq+Uq6BBsKgezjFur7dzzIr4ervK8KNEpuzG3ssKoS+3xzm9sXRaKK4FAoLCYenhImrORQqd1NzyEdD14mjQb9SUJP2VjXcAQYUNEeF7xUziQDYU/6asVx2ywyLStCvTdhth45LMWnHiw/ogDD16n/3uTaaeCjLPSjKtAQUjFrDq1ra2H+D+OjQYLFfU4+v3WcV4fg5D79+Y6u629YO2i5ja7X+QxOrAGxrmumlr55lndLKwOkZp6+scjMKuuMkU9sY8xiDPtKAlwLcR43BcVQPEv3C5EwwsELcJCNHw+b3KDxfB0jp2nlNLYmadRn5Nnrt/73TLaZpZV5G0zTDflRjebI7rMML5WxCjLSRtUHwnmxHOrYMsFE8jSQm1eas62qsQ6TyxJVfOKqoDlUSLnm4ezLsZjNA01Q7UHqKE8c3c2+dYTdq0xW9DXmLIxbW+GHM4juml/opvKri/Qeo4Hwlb3+rP/+HcD1iMcX2SammItgMPEoz9vx/CwpbSBmdeebRdANLxA0CLIDS8QtAgLMenrvNmHvAIzOAHzOtbPnI7HpM8yduQlqTGd4XdKMAvNa0IUIrFEM9hjgjom/62LRRwnljaJk4zDPzEWOJTKiVM55jbMBBxwkS264e8nODbmYBVwjjlMoYYMEkxzMOkj4zSK/UU9Tk/3CxyiF5n0biGMcdphSBSdbca+9tBsO+dWv5LA6yGWAhvzPuRwxFcs63IMhB3NpRsFslKjCsx/fT5jLOpBk94cQsimh1CzPRzHWeljNbp9iIYXCFoEueEFghZhISZ9VTRNejQ/CjCzap0phTXyBjHErtmqBNnIS2/kB8wcl5f+ovpnXWsdNOkZxtyNE3glAXOtJEVgWJV+Xnw06cmag8CgQpiLoNcjZNKDuVdrT34N+yIpaKTXKYr9Hv+6AE4D8nQ4wd4C2uSM4oC3GAlEDVNN7Tfp6xCfAbkMR/yS4ePHh58OyHJfKS7Y28w1JMw5hz7CT4bNCI0DutYx9X2vUs0cj4/CSS8aXiBoExai4UtwYpinY1X5M6vmudJ8ed50ahWgFUv95MSOJDVQVtvYedBpN3P+534SuZlVzXxvV1aT7w0fzL5ocMjdVYHjsdTHm1TYX6zpIAzzVOOszLqjVocuLZXRtH5hxZhbXtvYONYuwNhw8cWZf+0jOI+1PY+89pVHg/k0IOZYmPVOErQC0VLwpOI5v9JkvAl1wzF/rwOyvNmQ6JjGa9LmPYQsGcgetfTfgfwCG9QPiLqldIFA8LcecsMLBC2CMN4IBC2CaHiBoEWQG14gaBEW4qVfXh1A+N0wk7A3OoU+2w899DFERPSch55lt/3gD74hIiL6xV98rZXz0Mc9g4iItraY1aSuPHzf4P3c3nsBs8HceKenwBo9tb7KOd62tfPZVtbB/pvtF008OwHPPTLSlDNVo17MuFZ950mvtrIe/6sftrLyXB1PAhz2vQGn7GYdlWpcQ7Xc+j2v4mP88E8wE4/mya8KSK3F0ns7Qd64/cx/bjff+NPvgXPY9Jyjlz7r95xPIqLB3f/Qyjp/5Cf5GEs1HyCqcb30JlU55byG7XteHhERHV15EzDLuJ9Ebnpz0tFj4HTf3GHGm8P9d/Px2Sad2PMAdKGJMEGkafPyc3mtHn2HlVXMNbsNpDEnGUQ0UnUO44QbUG7ufhIz3hz8T1iZ/k2fdFM4yPQe4PVb37gkjDcCgUBBbniBoEVYiEmPKZim0R4mkHQ6bLJ2uz396SHASGB6kacFEabWWpM+NClfEg0SKqB5f1HkorlvhfNyGiFq4snAxFL8gyHTgPVLsBpMm9+YTOPMCsx3M65yaGTopIBqWZ6EJyKi2fDYjkv9euCsCsyxlyuSxV7Jr1sD2DWfAKGj1jFOU0RMTDFVfJ5WU+42TyupqJkQFSQx8ZJw+raxXExNRsQJmOwdU+EJhC8x0rKZv4daovnSvfF6wnvioipGz1xve0+BQPD/PRai4d3CCjVO4CmH2tyMO52mhk9SfAo2KZuQHDGKArRPFkhSeeuiBFuTEGzL3KRCwrRLN43YaHi/rAQLhLSmwBRRhw7MyC0DKb9QD280PDa5rArQ5jrdtZg123QTEc2GR3acz1WTSrQsHPaAck0P/AVC+YQdlqSdVoSkpU76cDgl1m0hbdYKdZaHoiqUZxJhcYtPw6OH89bWQgxO6CjWjlXn2kLOAX3L1aHUWl+9u+/a/T+DaHiBoEWQG14gaBEW7rRLtMmFZvzyMjt3Njc3iIjo0uW9hpz19TU7Nn3UHTJTh5Tc8hAFJuXbGHreGRkBllKnksp8IiUSsulWjW3OL0EgudQONNy1mGFFoKZtCvDSz0dsOpc6pm8+iVyTvtZ0WuUce8ozZhOuliuMLPhd52i0eVzAsVyGP5+f8utBnCp3XtLl2HLahfh5V52TqmyeG3xVKq0DE6414DyIEu3Ejf1r5Sxy1DTpHc4DWy7oF4V1+qRNeqfLT40mvemQFLgeanzVaL4iVQXPrKwNb8Pt623R8AJBiyA3vEDQIizGSw8wMcgemPQrK5xWuL29TUREd95xR+O7GxvrdtzVsXs0heqPKnajwNpfAAAgAElEQVTuEA3dYhsiJLNJUuBae+hZ1/MNmOEVeNFLbb4XEE/HZom22UZgXrMhx7sLbaoXOZvsVTFrjKsiZNJzA8RSM+66Jj3PodCpvtOZX9bZMTfaTDtKVmfA57EPzTBMQ4/KkzdR4euPHiJRbw2ttEx6c+hV6qY4g+fvIe+/B9j+zJj3YMZH6KXXKbsuIUtoXk26LExRn+c6TbmSOLxAIPBgMbz0WBmhH34JZM11OlBk0VfOm9UVaIlr/8ZWQZJoskt48iNN0oV1/W6amDs5Jc2za4CyCZ6TNt4KRShO7FvHuUvQmIjpOTvHZmPldMvn/P0cufd1EQvSOiHG55wdZzR4CVrdmZeOw9cBvvwCMvAqHV+vMNcAG15W5jt+K2Y64mOMZzoDEGL6WcrXRqpzL2Jq8uXj/GtbzwIOVCgqsuM6kJ/hZGyacYDO0v5EiHjS15Ibb63btx59TTYjyBlAKrjRUG3Pi4us21vPRCAQ/C2F3PACQYsgFFcCQYsgGl4gaBHkhhcIWoSFeOkH/SX7ntDtqlTDza1N+3dMo33xZ3yS80lE9InP+/yIiOhvHn67lbO2qp5NaYqeZo77mjr5KGLP/tbe5zL90/W3Q+6r9gBHePjYGMHEu9nDu7Xz6UCP9Nuws57DDGq+p+yRn5+pGPT8nGPR9336D1pZf/2br7KyJmMlYz5nT2yO3n+9Z5qxZ/p5X/lr9j/v/qUvYeot7aWOwHPtRBLKZn7Ax7/yd1jWG17AFFDac1yBB7l0ohqJ80lE9Kmv/D0r651veBHIUmveX+K06bWtXTteWlWp1ph6e8ez/1VERHTtr5jyrNCx5wJi0GmXU7Y7fRX1QdqrrTs+H64HOIdxtzF/p+WVaQ8F+nFr+zlW1vHBn/G6m9RZpMhy2mqZdFi+djd2Px6o2P7Q7my6ECOd2NERVzce7KvrZTblc/z8F3yhUFwJBAKFxTSThCdaqcfYfqqA2PJspuLEIyj8sH+b8tOsGCitnDjFEB6Gkwv6ARJhVhzG05291b+hYglYtrpSMso5z6uY8NN7qrPfJicnXlmjU46dT42GB5aaAppBmvmkHf/EptAeKtHaOEZyTawXCdSUWPhYdQIdj4yVUAeYeEqoubcxe1A1xZitslzH4StPzX8xGbNMbaVhdh05bbVq57MBp/DKjEP6z7R0up32UGafJmOO2nzrNmZuHb755MWezXgNTk+V1TgahbL2bjVTgUDwtx6LybSDTLBSlx5OoEzzFLLLrly9RkRED7//g3bbiz5DfT525RpPNFU+gLVVzMoC7WVSmINhRl8La/izwyRi8uNDz0PWKnWlnujzGb9HTc75KTw+U+/zk7NTr6TJCPLf9dMbc9adVsqaGaYMJI8VsMam5NPh6kOlEzJffDt7zCanCln7A+JAG+u4QqYdbT3NWFvOR7w2RiPHnWZe/vAYymy7qh4j6nJdRtrBdtKaDjpqZuwR3Vy+qsuOfY0aCcuhb6OLpyc7zsfDGGpM6dSHmEassO9kytfW0Ylaj7OzpnUcgmh4gaBFkBteIGgRFlMeC2GjKlZmyQyKOM7HHLZ6/Joy29/3fg6pGHzksat2vLaqTLNOxkU2A25wQ0libfrApJAcsLmPa86ldqsPTDVMVOte7vMZmF1jNkfHI2WCjc8DxTNg0ptCFjTrKiRp1KZnGTjGYo4m/U2fRJRG6K7UJJBRwEytPc6jGs3cuLFviIknAgdcrB1rNVwPBZJcmt+YN2WN4VUwq9U5yKCDC9VQxKJN+aBJX2MIzkN5HerJ7oXv9SfgIbXm/cX02TaEB69KY3BcHh4pp93RMa/LRRANLxC0CHLDCwQtwmKaSa6g6ad/CJvrxWzOHR/fICKi97+/aeK87+EP2HGvYzzUO3bbHZfZnMs0OWK4FshjdoGr2TVttbkbZC3kuHEUq4ywOOHMsDjtw9/Txq+7s/JxoWNjSv6rsYzzud8cPD9nb3iqjyGFY+j3Yhir7XHgkV9io02TaZYyj0EUs6mca6abWe5nvJlGsDY6ghMBcWU02LDjZLCqDwAaKJq/LW3bcdpXUZu0yxl7CbzumfMRRcB/D4iiZr26e+1g7LxZL49w6vAN0agTFcKwipYVIltFJqFCN2ItWNbpKZvv125cJyKigwPO5bgIouEFghZhIRp+aRk0vFWmkAUFbZ6Pj9RT6vSIY6wGDz/8N3Y86KkndbfLstdWWdOurhqnX8jBclEvMYTR8AHGG0fDq6d3Ahop8Wj46ja62Bi+vsjnMCNWNLOJ3zk2POMMvUTPPan9z/Su1vBRQMW7Gt7UMbBWjkGbFoVySI4DyzmN+Hup7jWYdlZZ1tIW/31J8RjWiUfDD3i/pKf2S3vMe4gaPoq1NRL5L/EInHmWKxD9uo6TV8fOA9dWDR13KtNFx9HgHg0fdAjDtzS34WzK8k9POWfh+vXr+nPfK8sH0fACQYsgN7xA0CII441A0CKIhhcIWgS54QWCFmEhXvoXvPCOJsF3oPV2rVMkq5rjpe/8g4cjIqKXf9WL7bee9MQHiIjogfvvtPvddy8zpVzaU97abo9jxZfvCTCcUJPxpoZ0WdKxW/Tkbm4/AxhO3mdlGf732enjdt/JyWN2fHL1r4mI6PTq/7LbXvCP32plvf2Hn8/Fe7lOm3S89FBVpjnYT4bstX3Zd/65lfX6b3g6yyr190r+/vYOH+PuJTXuLfEx/p2v+l0r67///Gcy80qmIiBZH+LgHY6dD3Ud/whSZF/0sp+1st7xi6+wsjJd794bcA7F8iqzIXU1E06d8Hm8/2NfFRERffAv/72Vk6ZqTmnGKdlZxseXpqZaDqI6l59n/3Ny9V1QNOlrJum2kyRy4/Rbd3wyMyBdY1m280wUisPrSAy44zeBTelo/3etrPMzdT2cQ/XlH/3Rn9rxH77rj4iI6PoN9tK/+a1/HgpTEZFoeIGgVViIhn/KnavNjU7tCj9nSuo7n4i9HY6xDvo6k65ijTWEzLLTrsryWt/gGDkiirBYwgA1KWavaQaXwPL4Cm0SyELrILdaR80nTf2ykphlGS69UM88o/iLQD38dA5ZWnpc5Hxcg3X4Ld3DLelxjBzRWeZzaGLdvQFzEWZ9znhMlpUGyqBWG7Fx6QksS8fhe31eox7w22V9rfk9cfiljUt2bDLosDgGQ99mLatA3bm73WhdzInwcAAEGZDwhJjecQGT1v7Wxfkic10MNRxy4dUErKhc9w/EluAXQTS8QNAiLETD37fHmrm2T1rQq/DAm9dKA+YVa0iD3Q3OnNreUFpgDTrPDnqsJVKd3RbH/nJIqrHDpxkjF5mnX1wwE6/JpReDBs+gd16WdfWnP6c7SXheldEwVfP9EedbB57TyLRa6nGO2YLwvpsuqXfw7iqUlwIGG6DNe0rbZ11+b087fG5q/V5uPm9GbxlqHnSPwQxYabMej1NtEVHSXK/UWUPDJIu95armOMgs0+zhhuc7qIA9iDwWWVQH/AGR2RbI2oOy3JnuJjw8Aw0P3I+mQ3CV+/sD+iAaXiBoEeSGFwhahIWY9PfssOlnTPmihBbIUE44yZVpNs6bJs72OpvGl7TMrW12Fq2tsYk6WFL7ZlmAShhNen3YkcOG0jTp64BJ74RZNJtJjH0LMpabdpQZapx3NwPbaBupFYVMet1KOfabg0nKJq9pt2yKd4i4MIWIqLd+BxERLW3xNsTK9l12nNlSVnzt4gNONVV3Vvr1RwqmfmLaQSewLxYImQohX5vnGk3X5muZr0AqCpah4jk0nyESSyPSv+6+re6v4rVlTH6/LHzdnU2UM+4cSmKnQzbpS03xjm20L4JoeIGgRZAbXiBoERZi0u9tcuaU4VifFdCNBZhR4rGyYQqPN7ULZJj9rjJRl/psVi4vcwx5MOjq3wt4ZcHzbWrAnQ4ysK/xujp84o4sNi0NvzhaqDGY9J2umm+33yTpJCLKunw886nupgKHUNXoxc/0d/yvLYMVjmd3tPk9gNr9lW2OYw/WLxMRUW+Vv4PorXIWY5aq361KiOPD0qSl9ryX/ghJ1uHtiYn/pxA/x5r86BYRktrjWfeZ3kS3YJQxX8PrpGleu2xHUWi35mZfA6QmbT2Vns46RET5jK+tqfbID4+5a1EOJJaZXo9efOtjRYiGFwhahIVo+BXIjS50m5RoCh1W4DmTV+opdj5q8qGdnvN3ljTTR3+Z478r6+ykinQ8G/nFEGXJ2Ui10Zox0E07cVP15CwDVMJFiZlN6nvQbIcS1PADNd+ltS3yobvCDs7pXD295xX3YpvlfIpKra37a36tvHc3Z7Rlmhuus8QZcxvb/Fsrm2qc9Zv5D0REccIWieFpi0CTYM86k2MQii1nHWDK0U7KGByMSYLn0dQ5NK2YKMK8CTPwnyPjrAtVf/uceeHst1vDcRV6LFWnDZ+xeCeTxn5EROfQXWeoaahHx5wrn+TstNscqPUYxH6HsA+i4QWCFkFueIGgRRDGG4GgRRANLxC0CHLDCwQtwkK89H/9tn9t3xPyQsUVT4ecHnhyztzaj1xVnshHHj+0277/9e+JiIi++9u+2Mq5fOeTiYjoCffebfd74IHLdry1rbzRyOG9efmzrIP02iO/ZWUlsareSiP2+GN9dKXTOktoAnjp7k+ysq489k5mONGe6y5UvXXAg52fKfab+Smz4Fx6zjdbWe992z+1so73P0JEbr/v0Rgbc6qqslWIp7/45T/FLDVv5nVf3VTsNCubHB3oQFVa1lWeXQyBr+9+LjPCXP9NPkYd742wwgzWqyhK55OIaOe+l1lZ+4/8EqyX9tJDym+cQPw+bqbMru++OCIiOrnxdqfi3f0k8vYegBTW9b3PhuN7G+wc3/RJbpqtlYGymKXm5Oo7mD3H94YMYnNd4352wt1invgxX25l/fE7/q2V8OGH309ERI+8/2G77/icY/LziaqiqyBt/bt+4X3CeCMQCBTkhhcIWoSFmPTZgE3Hcq5MtGoCKYOQWHI8VCbZ4wdNeqTHbrDpX2XK5F9aZzKFO+ZYlaeTOyJ/5VAUw/ZImVUVkkM4hpChIfIndcQxUmNpgg/YhGnCpSHKzPxUUtkyJykt6SSluM9r1Z/DKUqUjKUNfxLP+g6b+iZBqQOJNUi2EXlMXoSbl2JSkZ09+K+6ei9JAtVyaLKbxpRg0kdOko1OoPLqoov0k+d8hQxcXz4sBdJtjXkfCmjBdeL9Obg4Kk1aMTr1N4A8usFkqOcnB0RENBnx63BZ8L0T6wat4YrAJkTDCwQtwkI0fDoAraK9QlXK6ajTklMpj0fq6XfFo+Gv7POTreqop90m0GdNZpyOa2rU68ifWuto+FrXETvPu6gxDLVSjqH1tdUNUJyTl0C1ZLRXJ0AWucRWCula8W7Oskqg/op0M8feqocklIjWdlnDp1mqP0Gruzmg7mcDWJTSdFqhroi14Cjxy0pij1POo9WJkKbLpys/Gv3kt85uDV/DUUDQWoBCGJOGjDRlaPHlRsM3m6cSER1ev2rH5ycqpXY6ZkuXar6O00TJDV2nPoiGFwhaBLnhBYIWYSEmfQH9smeaM/vkjCvfbuyziXJwpGLOR6dNbu2DY/5O0jvW+7OZf37OJv1s2uzogXAolYy55aNRQhkBZ0gMVEtMWQTywRFmKs0o8deKxx2IjWsRSQe+X4NJHxuO+wD3Pk63ND3NA8doPqPAM7/G15Im5ZfD2x75nF7+ifkYYmtn4kaGb14fvckdtMLhN32dZ1zxdXOTA1hjfe7zOV8jszE72o4O1Kvp9es3vJKuXr1ux4eHKuZ+dM55GTE4CA2lgJj0AoHAC7nhBYIWYSEmfV4CndVMmTPHJ2yKX73OHsr9Q5UeeHLWJMA4PGFTJuoo8+bwmF8HzuA704nylHYDXAAJ0MoaiztElsF9KPw0RBHx71oLq4a4spOiqT6qJNBqCppWxKahA7TTohoJKgwTrX/aNaS2VoZCCTzI2NYqMq84cYDlF9JobaQBm4mAgWvIMKKgSd/0YrvvXrdLPBFKo/X+6G3uh3L9rwy19++wJ3jODRvtfM5m/BnE3PdvKJP92jU23RGPg0l/cn6mPzmCBakUlOl3wBCLsQ+i4QWCFmExGn6Gmlc9nc7OWcMfnbCWPh+pv0/nTW07AmdHqi2Ew0P+7uEhFOScKDnBZpKQ2WUe2K6zyONYivwaPibUWB6ecRwmhh4qoOGB6qnSjr+6YAElLEtl/hMgQCxmkIWl557gMQAPV61VRZz6nYmO9eNNNPNowEBmIloGtZmPwxqK/zEqzJc15zvuEFe8xxHnALPjKj1NbPHscSQGchYqJDUt1D6TMbeHOj7iwrDDAxVbPz3lIhjE+Zit2rF2/E0hU6/rEKTqhppJyEprQjS8QNAiyA0vELQIQnElELQIouEFghZBbniBoEVYiJf+j9/yGvuecHisvI5//BeP2L//yV99xI4/fFV53R+5Cum2V0YREdHevetWTr+vvO/PeMqT7H6f+vzn2vFDz3oqERHdeSdX0z3l6S9lGqJDpjSqtZe7qrB1UdNri9vWL32JlXV69T80cjCjGr3duKymIQJ/Ze3OL7Syjj7yy/YPpfbC5zP2kM8mPC7yJtXUE5/7aivrA+/5MaDxUnNPoXYf04tNbXwMXW13nvQPeF6P/ALTUulGE6G8ZeMRx/Vau/xSWK9fwT5fZjbU3Mbba/it9d3PUxRX+2/hOdnve/o4wZzQs766+zl257MbvwnnUM27crz0GCVoxuE39r7MyrrxKK9VMVMx+YPr7Jn/yCN8vT/2qKI6e+xxrnv/jtf9NyvrlV/6gJU1nCtZoxnH+ZdXuEHIxprmPIBWXj/84+8RiiuBQKCwEA1PkPFltKmb1cZjwyoT+xrigQY2T84h9MfeP+CMvWv7ary2wcUojqiqmdkVyvWyyWDBUvG6McRiDJeBxBSe+J+tceJpaBlj4QqwpZBagziQAYi10kazoU8Wx1Vt6rZDDTPB4jFjzNTzpPuFWk05MW2jRaPAxMzx1k35kVPs1CS7jBwNb2QHsiVhO/ekR60OWYt15e53EwrIqpuOVQ7KGIhaz06gfdSZir+XZbNYjIiow2kZ1NcFV1GHr5HVFeZVWF1taviLIBpeIGgRFqLhexnnfy911dNxc5XfPXa3mB76bKSedAdHTe3Qz/h5ZBhcSqDkPTg6sOPHr6t3or1LzHmHyHPWDmlsGkAiq0uA/caDGlo4m3fzKNS22FoT/mcr8rmZMsdOx28tdHSbaCfTDzBY5ac/l7+irLgxjgM5/nGMTDkm1S5gE0Weba60wHYD0Kxmup6svRo1vGXsCayr9SsEsiXB4jRyKyjrLiHF0bQ8L0p/7cVkyFl10zOV8Tk540y6GWSZVrph6HLmNx93N/jeMXyIlheRiDrgc+npFupJsB6iCdHwAkGLIDe8QNAiLMakT9ksGWiTfgPCCXubbNIfHSlWm5Ve89kz6LKpkmiTHp1/x0AEeHVfmbP3n+155zSbcYFDpF8Vsg6aQlBY4gmlISrnOWlCeEDb7P1WIKQFJn1iC2143yRD9htN8RwoCOmvsMOybkaSHMchzzcwW6fYxxcC8zlBAyZ91FyvC9lrfIeIjjbNWlRXQKbpExBgNaJiDrsqU74qeRu+OuaFDpfmfgr0KZj0o1M1HoNJPweaadIm/ZLneici2gWnc5KpMbIi1fBaVfmW8gKIhhcIWgS54QWCFmEhJn2c8nOko8frXf6pS30ONp4sK6/jyWqzjh09lrXmde8MIG4dcSxzorm7j4/2vXO6cZ0zm7bW14iIqKs/iYgiYAI08eQqEIh34s2RryuL1x71ynLqyk1MH//seNZvfbrqGNbQEk86wprjEOsn1OnfPL+bwU76UCaeLwKCWW3oSb9FHXvdzO9AxznmDtjadiAcRcwn0NVIe+dzEJbDq2NZqrmUVTNyQEQ0HTPZ6tmJyrAbQb17OWXGmkwf90rPz9uwC52Vestr+pP7EMwgC3M6nuv53T4Hv2h4gaBFWLyG151P1lDDDzgz6GRZaRKvhl9nDZ/3leOiSlAT89N7PLm1ht+/xh09enp+m2v8NEWHVmU0fOB5WKMTqr7o6WoojkN8aB7nFcb08bf0OJTRRqDhvfugBo4ucLQhE47xf6EGhXF8gdPOzcrzaXB0TIY9UZjfXmmtVs5hG2jlWGvtuvJntM3GEBvXjDU5ZnY61oK+HgJ9+KYj1PDKkTw6Yx67YsqZeJk+vtVe04IicjX8ypbqO7i6tWO3Dc9Z1smRchDOZ35nog+i4QWCFkFueIGgRRDGG4GgRRANLxC0CHLDCwQtwkK89O/73X9h3xPyqaoPHt5g7/nZda5y+9AV5c384OPs1fzeX/1ARET0qi95spVzouPCc/DSo/N3eaA81PfusUfzO779Ldat+sYff4WV9cT771Of991n9+30OX0x1gwvWIi1tff5VtbBtf8EwXPldcXKuxi8zpGnbnv10ucx88q132owrzgeaqgas0VrIGt99wuYpebGb3nezwJ8+eYTmWW2mRHm5OAtjQSBqmx6yYmIElN5B7kMa5c+l4/xOs+Lq/ea1WpERJUeY+35xuWvjIiIDq/8HLMDTZVnupiwh7qacz+EKlfe7Krgbfc+/9vtnD74+98Ea6V+K4YmnRFEKXRmrf0kInrKp7Csd/7CK62sxz/0KBERnR+Bl37Cc8h0+vTOZc4B+bvf+gdW1rt++iVW1uquShNf3du1+85mPO3JUK9BwdseeskPCuONQCBQWIiGn+QQ49RFClHNT7luwo/K1b56IO154vCrfS4sGeq4aA71LjEwgZh2bGfANIK4+vgVO95YVoU8lzY27LZl0KrdxGgsf51xhJ1UuIAb94DhRfXw+D2zD2ah+R7Yoee0b7uf8+3W2+gmup9mMZHDpGOOIRCnxmOPrMXSLFZSCLPLYA+3UvcvLAuOs+dTLmKZ6zh7mXOWG2J0yhZnrLXuYImvh24GOQ3amsln/pyLYgzt0XWcvBxzjkgKX+vqHJA02BAHdjbnAC7D7hJbop2+thJqqYcXCAQeLETDz+eceVTrksISNDzF/EQ0SnrQaU6lAxl7NDfMorwJmU2NxNHcnzt9eg69vk7U+9Xh/g3eATRaqrMDkyjQp46Q7dZ83cOYCuNQyrpvs6tb/VrVK8ubvObnjmNWV7/WwjJkk1WH7+0lvDdGiT7GQP88lx3YfA+tGNzZfqk5J+ipZ97X8wmf19mY35tnQzUuc85MQ2AufWp8Nl1gS4L39Vy/K49Pmh2OiYjyQ/6N6Fhdf9mQ5woVztTTh5WOA1x7YBnUU50rD9d0p8esRulAWapR7M/a80E0vEDQIsgNLxC0CItpF52DSa9jGRWY9GXEJn2tKZkTT1N7LMYwDCclGICOea8/Z4ESxgnEVE7OlDl3/RqXzKZgdy0vKVPe85ah5oqU2zoU5VbSQljOklwG7HGHorlZaOOODfzHGPnIIB3g329NvVwDI0xlSlFzKC4pUJY2iRP/MRbAHhNVZj2ajT/UfPQalJ7iGfhN0xo7HwNZ5JCbP0yGB1qOv7CkmDPdubn08PhKCH+Nj9RvnTzudwjPr/H13jlS10Y24e8DFyt1c3UMWc//6hmd8n1SryiHY7nKc61Tfu2IlzSJJZDGXgTR8AJBiyA3vEDQIizEpK/AXOI+bmAi+rLHPG5sJ9ZrWEfAq4xWnzHvpwHLeQKm/onuXnPtgGOx/QF75Nc0GST2ZXPmlbPZlZi+a7iUF8TWb5LGI2PO4vrUTW92sIYdzfO6MXA98rXpCOQ36TFrrfSY9CUuvuWI9y9+jpET02kIXnESLPk3rxqFZ15ziMNPlLk7H7GZnY95XExNLkiAiLRG3nm1z3jE85yNOX5/ckPJOrnCpKmI2Q3eNz1V807gkNFLn+nXkqQXqGE/Y49/PdQm/QjupyUgY9WHhhmOF0E0vEDQIiymtxz02rJq2Mkg4udMobXCzJOlNXfivtr5l3BWUQkaxbSOmwa03xDGxzOVnZUCe/Bgn/P7BzoOX885i2v3abzvdMhfHKwoJ0oEqVNIM21j9kFKO+Bps1lmfr63i5x2NXROMU5O1OrIUkM2tu7XNPkYNLxee+R0qzCmf0FnlvkMNFRluv7w3zP4T2LWYN6Mn1eg6SpNDV2c8bnA76Rmrqm/71qccsbavFJW2tEBa+rRCcuaaGfdJOS0O+TvDcbq+LICayt4rWJzXgb+da+H/LuV1vD1GI57Ag5xTXldeRzeIYiGFwhaBLnhBYIWQRhvBIIWQTS8QNAiyA0vELQIC/HS/+EbHuIosPa+lwV71/MZP2euHRTOJxHRN73+TyMioq//iqdZOR/R3svzlOUUa9ygkvoqjp7BK8pv/8QfW/flS17Nc1rTXs118G7es8HdPe7bXiciorsuMXvOJ7709XbnD73r31hZ6ztbREQU96AJYJfHVKsljmCply59kZV1fvVXmcXFMMugZz1CL71OS4WIxtadX2H/c/jYm3jddYSjLpueeSL22Jc5x3UvP/0fWVmP/smP8ry0lz6QPUyR7jEfQQTl3mf9E7vHB/7kdfarhZ5PCrkKPTinmY5KVNCt5dIn/MuIiOixd3yrlXN+oPgNzg4e42PCisxE89JDI8ZPfMXvMbPMm77IyhrP1bXz+EfYQ77/GP9+fKC85PEhe8tf844/s7J+9nnPsrI2tfO968tTIKK4o44v2+bz+mlvfh/P6+XM8tR/wqbzSUTUu3S3Hff37lGH2ufmrDtP/3phvBEIBApywwsELcJiEm8gWcS058FMyRmQJ4w0GeHpqNkSaDRlc3M+14k3WImGJAylMYf9UYcZmLZjU7EFRur+ECqe9PyxtdEngqwrV5gua66Tc5Y2mR5psLbO8zLEiClXNMGLCM1xrUyaK5j0sTM2yTTkRZTzGtr+6aU/8cY2Y1KvtygAAAT0SURBVMz9yTIlVBdW+uTFTkIRtPwqzbkJyIIknqLQvdjxIIChaaZ7tE8hTfaS/tzf5+rGoaYyG06hvVTCSTZJpNZ7PPTrtIcfZfP8+Fyd+9NjToYZnfN52dZtobbv9Fel9e/k8x2baxYveDgHkZ5ivOm3vNN1Nt/TFTVO+rwtSvnqKfXCXdjtDCAaXiBoERau4Wur4fmJNoWswltq+Alr+FxrogqIJRN0jJRNhxcC03RHOp21gMIRrHGvda11Wfhrlh9/jDU8aa1agTWQQiF91NdzzPxLnTt0zVqTwlxjbNpotHbokY7zrYzTDhJykSLKNGMMaPiqyBv7xjUfAxa/lFqbVYV/XsWInWG5Xq8Ii1dimJemlT4/49p2gxug4cczdawT0PBpl+mfOqlyZB2O/OcQNfw1nVKbz0Hrlnydre0oDb6yw5oc0b+LKacTXZiFhT6o4UnTtsVr/pTfBDR8sqzGaUjDa/LKQP2TF6LhBYIWQW54gaBFWAzFVcXPEWMxjqZsd5yNChgrE+982GQEnUNXEWMxJ8hLn4OppFltiwCVFFqbxnCrwII7R9NZy+0M/Yynh/vsUOpoi7GC6ednwMqrue+zTd62t8f7Tk6ZddX4xBKH/x37n9+6hn0+ZlmlNiMrOK6ybI6L3C9rCvXgRkYKi59CDXahmYnNZ0MWMMsa5yvy8acpylLbT4bN83j9hOd6fKYW/OiM5xklfBLSVG0f+i16OjiF2vpYxeo399h03lpnJ+y9O6oi8u5dztVA7DzlHpZ1pmrmizFfOyWY94YDv7Pul9W/h2X1dD5Id5vnEvWBSbmrx4nUwwsEAg/khhcIWoQFmfRs+hnvOJr0p0M2cc6GyqQf+kx6jLHqJgeQaUpRDh5o3c+nDjzCSqC4rbXtXIBZGiNzqp5zZ+xvPHB0wCZqps33/IRtx8l1jukv36lkLM/BRH0GDydHTOCQ9dQxYv+NCggyYh1JqIsA0cSIZVlaKjDjC/DYm/UoAp71yZiPweQ2ZGA6JkBaYTzvRaAJyHjC8zKe8DhmL3XW4fj2PFfbT0bN9knXuc8EXdlXv/X4PsvGpopWlyX+ZiJH57yGa5vKdL7jCU+w2570ADcavWtLmfx3bULKNGDvQf7e8Egdy2TIr31TMO8jTf7S2eC0bUT/XpbVXVfe/x6Y/zWsu6FCC3T48kI0vEDQIixEw2MDxNLG4fnpOwdnW67Hucd5hNrJIKr8Y6ugQ/yOSHhpP5utqoiI5oZ2q/Q7tGYzaB6onUyTmp1HGVg42Zp6uncnzTwDIqISZJlOTdjDMkJiSqPhA7RUmDdg1s511N2+hncy7fR6xM7iwjnW8ynKwDFCTL8slY6poQFiAt7TUjt850XzREJaBg0nal1OwFrM55hnoK2SAGX7DKzDSGfoDZY4xr2xxY6ytU0lZHnDby3017l4ZVYqi2Me8VrEMV5H6viSJb+1kCyxLNNKKunzvJDgtNTNNaMgf1oTouEFghZBbniBoEUQiiuBoEUQDS8QtAhywwsELYLc8AJBiyA3vEDQIsgNLxC0CHLDCwQtgtzwAkGLIDe8QNAiyA0vELQIcsMLBC2C3PACQYsgN7xA0CLIDS8QtAhywwsELYLc8AJBiyA3vEDQIsgNLxC0CHLDCwQtgtzwAkGLIDe8QNAiyA0vELQIcsMLBC2C3PACQYvwvwGyi7vF5JhkWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 144 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ds_6wxWflF23"
   },
   "source": [
    "## Implement the patch encoding layer\n",
    "\n",
    "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
    "vector of size `projection_dim`. In addition, it adds a learnable position\n",
    "embedding to the projected vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L20K-tyHlF23"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vssO606QlF24"
   },
   "source": [
    "## Build the ViT model\n",
    "\n",
    "The ViT model consists of multiple Transformer blocks,\n",
    "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
    "applied to the sequence of patches. The Transformer blocks produce a\n",
    "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
    "classifier head with softmax to produce the final class probabilities output.\n",
    "\n",
    "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
    "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
    "as the image representation, all the outputs of the final Transformer block are\n",
    "reshaped with `layers.Flatten()` and used as the image\n",
    "representation input to the classifier head.\n",
    "Note that the `layers.GlobalAveragePooling1D` layer\n",
    "could also be used instead to aggregate the outputs of the Transformer block,\n",
    "especially when the number of patches and the projection dimensions are large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eZzsK1RPndDs"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Lint as: python3\n",
    "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Keras-based attention layer.\"\"\"\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "# pylint: disable=g-classes-have-attributes\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from keras import constraints\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.engine.base_layer import Layer\n",
    "from keras.layers import advanced_activations\n",
    "from keras.layers import core\n",
    "from keras.layers import einsum_dense\n",
    "from keras.utils import tf_utils\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "\n",
    "def _build_attention_equation(rank, attn_axes):\n",
    "  \"\"\"Builds einsum equations for the attention computation.\n",
    "\n",
    "  Query, key, value inputs after projection are expected to have the shape as:\n",
    "  `(bs, <non-attention dims>, <attention dims>, num_heads, channels)`.\n",
    "  `bs` and `<non-attention dims>` are treated as `<batch dims>`.\n",
    "\n",
    "  The attention operations can be generalized:\n",
    "  (1) Query-key dot product:\n",
    "  `(<batch dims>, <query attention dims>, num_heads, channels), (<batch dims>,\n",
    "  <key attention dims>, num_heads, channels) -> (<batch dims>,\n",
    "  num_heads, <query attention dims>, <key attention dims>)`\n",
    "  (2) Combination:\n",
    "  `(<batch dims>, num_heads, <query attention dims>, <key attention dims>),\n",
    "  (<batch dims>, <value attention dims>, num_heads, channels) -> (<batch dims>,\n",
    "  <query attention dims>, num_heads, channels)`\n",
    "\n",
    "  Args:\n",
    "    rank: Rank of query, key, value tensors.\n",
    "    attn_axes: List/tuple of axes, `[-1, rank)`,\n",
    "      that attention will be applied to.\n",
    "\n",
    "  Returns:\n",
    "    Einsum equations.\n",
    "  \"\"\"\n",
    "  target_notation = _CHR_IDX[:rank]\n",
    "  # `batch_dims` includes the head dim.\n",
    "  batch_dims = tuple(np.delete(range(rank), attn_axes + (rank - 1,)))\n",
    "  letter_offset = rank\n",
    "  source_notation = \"\"\n",
    "  for i in range(rank):\n",
    "    if i in batch_dims or i == rank - 1:\n",
    "      source_notation += target_notation[i]\n",
    "    else:\n",
    "      source_notation += _CHR_IDX[letter_offset]\n",
    "      letter_offset += 1\n",
    "\n",
    "  product_notation = \"\".join([target_notation[i] for i in batch_dims] +\n",
    "                             [target_notation[i] for i in attn_axes] +\n",
    "                             [source_notation[i] for i in attn_axes])\n",
    "  dot_product_equation = \"%s,%s->%s\" % (source_notation, target_notation,\n",
    "                                        product_notation)\n",
    "  attn_scores_rank = len(product_notation)\n",
    "  combine_equation = \"%s,%s->%s\" % (product_notation, source_notation,\n",
    "                                    target_notation)\n",
    "  return dot_product_equation, combine_equation, attn_scores_rank\n",
    "\n",
    "\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "  \"\"\"Builds an einsum equation for projections inside multi-head attention.\"\"\"\n",
    "  input_str = \"\"\n",
    "  kernel_str = \"\"\n",
    "  output_str = \"\"\n",
    "  bias_axes = \"\"\n",
    "  letter_offset = 0\n",
    "  for i in range(free_dims):\n",
    "    char = _CHR_IDX[i + letter_offset]\n",
    "    input_str += char\n",
    "    output_str += char\n",
    "\n",
    "  letter_offset += free_dims\n",
    "  for i in range(bound_dims):\n",
    "    char = _CHR_IDX[i + letter_offset]\n",
    "    input_str += char\n",
    "    kernel_str += char\n",
    "\n",
    "  letter_offset += bound_dims\n",
    "  for i in range(output_dims):\n",
    "    char = _CHR_IDX[i + letter_offset]\n",
    "    kernel_str += char\n",
    "    output_str += char\n",
    "    bias_axes += char\n",
    "  equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "  return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "  return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)\n",
    "\n",
    "\n",
    "@keras_export(\"keras.layers.MultiHeadAttention\")\n",
    "class MultiHeadAttention(Layer):\n",
    "  \"\"\"MultiHeadAttention layer.\n",
    "\n",
    "  This is an implementation of multi-headed attention as described in the paper\n",
    "  \"Attention is all you Need\" (Vaswani et al., 2017).\n",
    "  If `query`, `key,` `value` are the same, then\n",
    "  this is self-attention. Each timestep in `query` attends to the\n",
    "  corresponding sequence in `key`, and returns a fixed-width vector.\n",
    "\n",
    "  This layer first projects `query`, `key` and `value`. These are\n",
    "  (effectively) a list of tensors of length `num_attention_heads`, where the\n",
    "  corresponding shapes are `(batch_size, <query dimensions>, key_dim)`,\n",
    "  `(batch_size, <key/value dimensions>, key_dim)`,\n",
    "  `(batch_size, <key/value dimensions>, value_dim)`.\n",
    "\n",
    "  Then, the query and key tensors are dot-producted and scaled. These are\n",
    "  softmaxed to obtain attention probabilities. The value tensors are then\n",
    "  interpolated by these probabilities, then concatenated back to a single\n",
    "  tensor.\n",
    "\n",
    "  Finally, the result tensor with the last dimension as value_dim can take an\n",
    "  linear projection and return.\n",
    "\n",
    "  When using MultiHeadAttention inside a custom Layer, the custom Layer must\n",
    "  implement `build()` and call MultiHeadAttention's `_build_from_signature()`.\n",
    "  This enables weights to be restored correctly when the model is loaded.\n",
    "  TODO(b/172609172): link to documentation about calling custom build functions\n",
    "  when used in a custom Layer.\n",
    "\n",
    "  Examples:\n",
    "\n",
    "  Performs 1D cross-attention over two sequence inputs with an attention mask.\n",
    "  Returns the additional attention weights over heads.\n",
    "\n",
    "  >>> layer = MultiHeadAttention(num_heads=2, key_dim=2)\n",
    "  >>> target = tf.keras.Input(shape=[8, 16])\n",
    "  >>> source = tf.keras.Input(shape=[4, 16])\n",
    "  >>> output_tensor, weights = layer(target, source,\n",
    "  ...                                return_attention_scores=True)\n",
    "  >>> print(output_tensor.shape)\n",
    "  (None, 8, 16)\n",
    "  >>> print(weights.shape)\n",
    "  (None, 2, 8, 4)\n",
    "\n",
    "  Performs 2D self-attention over a 5D input tensor on axes 2 and 3.\n",
    "\n",
    "  >>> layer = MultiHeadAttention(num_heads=2, key_dim=2, attention_axes=(2, 3))\n",
    "  >>> input_tensor = tf.keras.Input(shape=[5, 3, 4, 16])\n",
    "  >>> output_tensor = layer(input_tensor, input_tensor)\n",
    "  >>> print(output_tensor.shape)\n",
    "  (None, 5, 3, 4, 16)\n",
    "\n",
    "  Args:\n",
    "    num_heads: Number of attention heads.\n",
    "    key_dim: Size of each attention head for query and key.\n",
    "    value_dim: Size of each attention head for value.\n",
    "    dropout: Dropout probability.\n",
    "    use_bias: Boolean, whether the dense layers use bias vectors/matrices.\n",
    "    output_shape: The expected shape of an output tensor, besides the batch and\n",
    "      sequence dims. If not specified, projects back to the key feature dim.\n",
    "    attention_axes: axes over which the attention is applied. `None` means\n",
    "      attention over all axes, but batch, heads, and features.\n",
    "    kernel_initializer: Initializer for dense layer kernels.\n",
    "    bias_initializer: Initializer for dense layer biases.\n",
    "    kernel_regularizer: Regularizer for dense layer kernels.\n",
    "    bias_regularizer: Regularizer for dense layer biases.\n",
    "    activity_regularizer: Regularizer for dense layer activity.\n",
    "    kernel_constraint: Constraint for dense layer kernels.\n",
    "    bias_constraint: Constraint for dense layer kernels.\n",
    "\n",
    "  Call arguments:\n",
    "    query: Query `Tensor` of shape `(B, T, dim)`.\n",
    "    value: Value `Tensor` of shape `(B, S, dim)`.\n",
    "    key: Optional key `Tensor` of shape `(B, S, dim)`. If not given, will use\n",
    "      `value` for both `key` and `value`, which is the most common case.\n",
    "    attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
    "      attention to certain positions. The boolean mask specifies which query\n",
    "      elements can attend to which key elements, 1 indicates attention and 0\n",
    "      indicates no attention. Broadcasting can happen for the missing batch\n",
    "      dimensions and the head dimension.\n",
    "    return_attention_scores: A boolean to indicate whether the output should\n",
    "      be `(attention_output, attention_scores)` if `True`, or `attention_output`\n",
    "      if `False`. Defaults to `False`.\n",
    "    training: Python boolean indicating whether the layer should behave in\n",
    "      training mode (adding dropout) or in inference mode (no dropout).\n",
    "      Defaults to either using the training mode of the parent layer/model,\n",
    "      or False (inference) if there is no parent layer.\n",
    "\n",
    "  Returns:\n",
    "    attention_output: The result of the computation, of shape `(B, T, E)`,\n",
    "      where `T` is for target sequence shapes and `E` is the query input last\n",
    "      dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "      are project to the shape specified by `output_shape`.\n",
    "    attention_scores: [Optional] multi-head attention coefficients over\n",
    "      attention axes.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_heads,\n",
    "               key_dim,\n",
    "               value_dim=None,\n",
    "               dropout=0.0,\n",
    "               use_bias=True,\n",
    "               output_shape=None,\n",
    "               attention_axes=None,\n",
    "               kernel_initializer=\"glorot_uniform\",\n",
    "               bias_initializer=\"zeros\",\n",
    "               kernel_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               activity_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               **kwargs):\n",
    "    super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "    self._num_heads = num_heads\n",
    "    self._key_dim = key_dim\n",
    "    self._value_dim = value_dim if value_dim else key_dim\n",
    "    self._dropout = dropout\n",
    "    self._use_bias = use_bias\n",
    "    self._output_shape = output_shape\n",
    "    self._kernel_initializer = initializers.get(kernel_initializer)\n",
    "    self._bias_initializer = initializers.get(bias_initializer)\n",
    "    self._kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "    self._bias_regularizer = regularizers.get(bias_regularizer)\n",
    "    self._activity_regularizer = regularizers.get(activity_regularizer)\n",
    "    self._kernel_constraint = constraints.get(kernel_constraint)\n",
    "    self._bias_constraint = constraints.get(bias_constraint)\n",
    "    if attention_axes is not None and not isinstance(attention_axes,\n",
    "                                                     collections.abc.Sized):\n",
    "      self._attention_axes = (attention_axes,)\n",
    "    else:\n",
    "      self._attention_axes = attention_axes\n",
    "    self._built_from_signature = False\n",
    "    self._query_shape, self._key_shape, self._value_shape = None, None, None\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        \"num_heads\": self._num_heads,\n",
    "        \"key_dim\": self._key_dim,\n",
    "        \"value_dim\": self._value_dim,\n",
    "        \"dropout\": self._dropout,\n",
    "        \"use_bias\": self._use_bias,\n",
    "        \"output_shape\": self._output_shape,\n",
    "        \"attention_axes\": self._attention_axes,\n",
    "        \"kernel_initializer\":\n",
    "            initializers.serialize(self._kernel_initializer),\n",
    "        \"bias_initializer\":\n",
    "            initializers.serialize(self._bias_initializer),\n",
    "        \"kernel_regularizer\":\n",
    "            regularizers.serialize(self._kernel_regularizer),\n",
    "        \"bias_regularizer\":\n",
    "            regularizers.serialize(self._bias_regularizer),\n",
    "        \"activity_regularizer\":\n",
    "            regularizers.serialize(self._activity_regularizer),\n",
    "        \"kernel_constraint\":\n",
    "            constraints.serialize(self._kernel_constraint),\n",
    "        \"bias_constraint\":\n",
    "            constraints.serialize(self._bias_constraint),\n",
    "        \"query_shape\": self._query_shape,\n",
    "        \"key_shape\": self._key_shape,\n",
    "        \"value_shape\": self._value_shape,\n",
    "    }\n",
    "    base_config = super(MultiHeadAttention, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    # If the layer has a different build() function from the Keras default,\n",
    "    # we need to trigger the customized build to create weights.\n",
    "    query_shape = config.pop(\"query_shape\")\n",
    "    key_shape = config.pop(\"key_shape\")\n",
    "    value_shape = config.pop(\"value_shape\")\n",
    "    layer = cls(**config)\n",
    "    if None in [query_shape, key_shape, value_shape]:\n",
    "      logging.warning(\n",
    "          \"One of dimensions of the input shape is missing. It should have been\"\n",
    "          \" memorized when the layer was serialized. \"\n",
    "          \"%s is created without weights.\",\n",
    "          str(cls))\n",
    "    else:\n",
    "      layer._build_from_signature(query_shape, value_shape, key_shape)  # pylint: disable=protected-access\n",
    "    return layer\n",
    "\n",
    "  def _build_from_signature(self, query, value, key=None):\n",
    "    \"\"\"Builds layers and variables.\n",
    "\n",
    "    Once the method is called, self._built_from_signature will be set to True.\n",
    "\n",
    "    Args:\n",
    "      query: Query tensor or TensorShape.\n",
    "      value: Value tensor or TensorShape.\n",
    "      key: Key tensor or TensorShape.\n",
    "    \"\"\"\n",
    "    self._built_from_signature = True\n",
    "    if hasattr(query, \"shape\"):\n",
    "      self._query_shape = tf.TensorShape(query.shape)\n",
    "    else:\n",
    "      self._query_shape = tf.TensorShape(query)\n",
    "    if hasattr(value, \"shape\"):\n",
    "      self._value_shape = tf.TensorShape(value.shape)\n",
    "    else:\n",
    "      self._value_shape = tf.TensorShape(value)\n",
    "    if key is None:\n",
    "      self._key_shape = self._value_shape\n",
    "    elif hasattr(key, \"shape\"):\n",
    "      self._key_shape = tf.TensorShape(key.shape)\n",
    "    else:\n",
    "      self._key_shape = tf.TensorShape(key)\n",
    "\n",
    "    common_kwargs = dict(\n",
    "        kernel_initializer=self._kernel_initializer,\n",
    "        bias_initializer=self._bias_initializer,\n",
    "        kernel_regularizer=self._kernel_regularizer,\n",
    "        bias_regularizer=self._bias_regularizer,\n",
    "        activity_regularizer=self._activity_regularizer,\n",
    "        kernel_constraint=self._kernel_constraint,\n",
    "        bias_constraint=self._bias_constraint)\n",
    "    # Any setup work performed only once should happen in an `init_scope`\n",
    "    # to avoid creating symbolic Tensors that will later pollute any eager\n",
    "    # operations.\n",
    "    with tf_utils.maybe_init_scope(self):\n",
    "      free_dims = self._query_shape.rank - 1\n",
    "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "          free_dims, bound_dims=1, output_dims=2)\n",
    "      self._query_dense = einsum_dense.EinsumDense(\n",
    "          einsum_equation,\n",
    "          output_shape=_get_output_shape(output_rank - 1,\n",
    "                                         [self._num_heads, self._key_dim]),\n",
    "          bias_axes=bias_axes if self._use_bias else None,\n",
    "          name=\"query\",\n",
    "          **common_kwargs)\n",
    "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "          self._key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "      self._key_dense = einsum_dense.EinsumDense(\n",
    "          einsum_equation,\n",
    "          output_shape=_get_output_shape(output_rank - 1,\n",
    "                                         [self._num_heads, self._key_dim]),\n",
    "          bias_axes=bias_axes if self._use_bias else None,\n",
    "          name=\"key\",\n",
    "          **common_kwargs)\n",
    "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "          self._value_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "      self._value_dense = einsum_dense.EinsumDense(\n",
    "          einsum_equation,\n",
    "          output_shape=_get_output_shape(output_rank - 1,\n",
    "                                         [self._num_heads, self._value_dim]),\n",
    "          bias_axes=bias_axes if self._use_bias else None,\n",
    "          name=\"value\",\n",
    "          **common_kwargs)\n",
    "\n",
    "      # Builds the attention computations for multi-head dot product attention.\n",
    "      # These computations could be wrapped into the keras attention layer once\n",
    "      # it support mult-head einsum computations.\n",
    "      self._build_attention(output_rank)\n",
    "      self._output_dense = self._make_output_dense(\n",
    "          free_dims, common_kwargs, \"attention_output\")\n",
    "\n",
    "  def _make_output_dense(self, free_dims, common_kwargs, name=None):\n",
    "    \"\"\"Builds the output projection matrix.\n",
    "\n",
    "    Args:\n",
    "      free_dims: Number of free dimensions for einsum equation building.\n",
    "      common_kwargs: Common keyword arguments for einsum layer.\n",
    "      name: Name for the projection layer.\n",
    "\n",
    "    Returns:\n",
    "      Projection layer.\n",
    "    \"\"\"\n",
    "    if self._output_shape:\n",
    "      if not isinstance(self._output_shape, collections.abc.Sized):\n",
    "        output_shape = [self._output_shape]\n",
    "      else:\n",
    "        output_shape = self._output_shape\n",
    "    else:\n",
    "      output_shape = [self._query_shape[-1]]\n",
    "    einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "        free_dims, bound_dims=2, output_dims=len(output_shape))\n",
    "    return einsum_dense.EinsumDense(\n",
    "        einsum_equation,\n",
    "        output_shape=_get_output_shape(output_rank - 1, output_shape),\n",
    "        bias_axes=bias_axes if self._use_bias else None,\n",
    "        name=name,\n",
    "        **common_kwargs)\n",
    "\n",
    "  def _build_attention(self, rank):\n",
    "    \"\"\"Builds multi-head dot-product attention computations.\n",
    "\n",
    "    This function builds attributes necessary for `_compute_attention` to\n",
    "    costomize attention computation to replace the default dot-product\n",
    "    attention.\n",
    "\n",
    "    Args:\n",
    "      rank: the rank of query, key, value tensors.\n",
    "    \"\"\"\n",
    "    if self._attention_axes is None:\n",
    "      self._attention_axes = tuple(range(1, rank - 2))\n",
    "    else:\n",
    "      self._attention_axes = tuple(self._attention_axes)\n",
    "    self._dot_product_equation, self._combine_equation, attn_scores_rank = (\n",
    "        _build_attention_equation(rank, attn_axes=self._attention_axes))\n",
    "    norm_axes = tuple(\n",
    "        range(attn_scores_rank - len(self._attention_axes), attn_scores_rank))\n",
    "    self._softmax = advanced_activations.Softmax(axis=norm_axes)\n",
    "    self._dropout_layer = core.Dropout(rate=self._dropout)\n",
    "\n",
    "  def _masked_softmax(self, attention_scores, attention_mask=None):\n",
    "    # Normalize the attention scores to probabilities.\n",
    "    # `attention_scores` = [B, N, T, S]\n",
    "    if attention_mask is not None:\n",
    "      # The expand dim happens starting from the `num_heads` dimension,\n",
    "      # (<batch_dims>, num_heads, <query_attention_dims, key_attention_dims>)\n",
    "      mask_expansion_axis = -len(self._attention_axes) * 2 - 1\n",
    "      for _ in range(len(attention_scores.shape) - len(attention_mask.shape)):\n",
    "        attention_mask = tf.expand_dims(\n",
    "            attention_mask, axis=mask_expansion_axis)\n",
    "    return self._softmax(attention_scores, attention_mask)\n",
    "\n",
    "  def _compute_attention(self,\n",
    "                         query,\n",
    "                         key,\n",
    "                         value,\n",
    "                         attention_mask=None,\n",
    "                         training=None):\n",
    "    \"\"\"Applies Dot-product attention with query, key, value tensors.\n",
    "\n",
    "    This function defines the computation inside `call` with projected\n",
    "    multi-head Q, K, V inputs. Users can override this function for customized\n",
    "    attention implementation.\n",
    "\n",
    "    Args:\n",
    "      query: Projected query `Tensor` of shape `(B, T, N, key_dim)`.\n",
    "      key: Projected key `Tensor` of shape `(B, T, N, key_dim)`.\n",
    "      value: Projected value `Tensor` of shape `(B, T, N, value_dim)`.\n",
    "      attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
    "        attention to certain positions.\n",
    "      training: Python boolean indicating whether the layer should behave in\n",
    "        training mode (adding dropout) or in inference mode (doing nothing).\n",
    "\n",
    "    Returns:\n",
    "      attention_output: Multi-headed outputs of attention computation.\n",
    "      attention_scores: Multi-headed attention weights.\n",
    "    \"\"\"\n",
    "    # Note: Applying scalar multiply at the smaller end of einsum improves\n",
    "    # XLA performance, but may introduce slight numeric differences in\n",
    "    # the Transformer attention head.\n",
    "    query = tf.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "    # Take the dot product between \"query\" and \"key\" to get the raw\n",
    "    # attention scores.\n",
    "    attention_scores = tf.einsum(self._dot_product_equation, key,\n",
    "                                               query)\n",
    "\n",
    "    attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "    # This is actually dropping out entire tokens to attend to, which might\n",
    "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "    attention_scores_dropout = self._dropout_layer(\n",
    "        attention_scores, training=training)\n",
    "\n",
    "    # `context_layer` = [B, T, N, H]\n",
    "    attention_output = tf.einsum(self._combine_equation,\n",
    "                                               attention_scores_dropout, value)\n",
    "    return attention_output, attention_scores\n",
    "\n",
    "  def call(self,\n",
    "           query,\n",
    "           value,\n",
    "           key=None,\n",
    "           attention_mask=None,\n",
    "           return_attention_scores=False,\n",
    "           training=None):\n",
    "    if not self._built_from_signature:\n",
    "      self._build_from_signature(query=query, value=value, key=key)\n",
    "    if key is None:\n",
    "      key = value\n",
    "\n",
    "    #   N = `num_attention_heads`\n",
    "    #   H = `size_per_head`\n",
    "    # `query` = [B, T, N ,H]\n",
    "    query = self._query_dense(query)\n",
    "\n",
    "    # `key` = [B, S, N, H]\n",
    "    key = self._key_dense(key)\n",
    "\n",
    "    # `value` = [B, S, N, H]\n",
    "    value = self._value_dense(value)\n",
    "\n",
    "    attention_output, attention_scores = self._compute_attention(\n",
    "        query, key, value, attention_mask, training)\n",
    "    attention_output = self._output_dense(attention_output)\n",
    "\n",
    "    if return_attention_scores:\n",
    "      return attention_output, attention_scores\n",
    "    return attention_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AhsnrfN7lF24"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSmzFg42lF24"
   },
   "source": [
    "## Compile, train, and evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " data_augmentation (Sequential)  (None, 72, 72, 3)   7           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " patches_1 (Patches)            (None, None, 108)    0           ['data_augmentation[0][0]']      \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 144, 64)      16192       ['patches_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 144, 64)     128         ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 144, 64)     66368       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 144, 64)      0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 144, 64)     128         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 144, 128)     8320        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 144, 128)     0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 144, 64)      8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 144, 64)      0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 144, 64)      0           ['dropout_1[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 144, 64)     128         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 144, 64)     66368       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 144, 64)      0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 144, 64)     128         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 144, 128)     8320        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 144, 128)     0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 144, 64)      8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 144, 64)      0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 144, 64)      0           ['dropout_3[0][0]',              \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 144, 64)     128         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 144, 64)     66368       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 144, 64)      0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 144, 64)     128         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 144, 128)     8320        ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 144, 128)     0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 144, 64)      8256        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 144, 64)      0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 144, 64)      0           ['dropout_5[0][0]',              \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 144, 64)     128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " multi_head_attention_3 (MultiH  (None, 144, 64)     66368       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 144, 64)      0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 144, 64)     128         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 144, 128)     8320        ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 144, 128)     0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 144, 64)      8256        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 144, 64)      0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 144, 64)      0           ['dropout_7[0][0]',              \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 144, 64)     128         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 144, 64)     66368       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 144, 64)      0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 144, 64)     128         ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 144, 128)     8320        ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 144, 128)     0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 144, 64)      8256        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 144, 64)      0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 144, 64)      0           ['dropout_9[0][0]',              \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 144, 64)     128         ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 144, 64)     66368       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 144, 64)      0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 144, 64)     128         ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 144, 128)     8320        ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 144, 128)     0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 144, 64)      8256        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 144, 64)      0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 144, 64)      0           ['dropout_11[0][0]',             \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 144, 64)     128         ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 144, 64)     66368       ['layer_normalization_12[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 144, 64)      0           ['multi_head_attention_6[0][0]', \n",
      "                                                                  'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 144, 64)     128         ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 144, 128)     8320        ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 144, 128)     0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 144, 64)      8256        ['dropout_12[0][0]']             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 144, 64)      0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 144, 64)      0           ['dropout_13[0][0]',             \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 144, 64)     128         ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 144, 64)     66368       ['layer_normalization_14[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 144, 64)      0           ['multi_head_attention_7[0][0]', \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 144, 64)     128         ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 144, 128)     8320        ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 144, 128)     0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 144, 64)      8256        ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 144, 64)      0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 144, 64)      0           ['dropout_15[0][0]',             \n",
      "                                                                  'add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 144, 64)     128         ['add_15[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 9216)         0           ['layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 9216)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 2048)         18876416    ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 2048)         0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 1024)         2098176     ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 1024)         0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 100)          102500      ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,759,019\n",
      "Trainable params: 21,759,012\n",
      "Non-trainable params: 7\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_vit_classifier().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21759019"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_vit_classifier().count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ujQwZ1-lF25",
    "outputId": "6443f9a2-76a2-4419-87f3-76ce589ac736",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "352/352 [==============================] - 66s 105ms/step - loss: 4.4497 - accuracy: 0.0448 - top-5-accuracy: 0.1620 - val_loss: 3.9296 - val_accuracy: 0.0938 - val_top-5-accuracy: 0.2996\n",
      "Epoch 2/70\n",
      "352/352 [==============================] - 36s 101ms/step - loss: 3.9234 - accuracy: 0.0942 - top-5-accuracy: 0.2961 - val_loss: 3.5647 - val_accuracy: 0.1470 - val_top-5-accuracy: 0.4106\n",
      "Epoch 3/70\n",
      "352/352 [==============================] - 35s 101ms/step - loss: 3.6713 - accuracy: 0.1313 - top-5-accuracy: 0.3705 - val_loss: 3.2874 - val_accuracy: 0.2090 - val_top-5-accuracy: 0.4832\n",
      "Epoch 4/70\n",
      "352/352 [==============================] - 36s 103ms/step - loss: 3.4658 - accuracy: 0.1672 - top-5-accuracy: 0.4347 - val_loss: 3.1224 - val_accuracy: 0.2264 - val_top-5-accuracy: 0.5216\n",
      "Epoch 5/70\n",
      "352/352 [==============================] - 35s 100ms/step - loss: 3.2824 - accuracy: 0.2001 - top-5-accuracy: 0.4831 - val_loss: 2.9786 - val_accuracy: 0.2546 - val_top-5-accuracy: 0.5576\n",
      "Epoch 6/70\n",
      "352/352 [==============================] - 35s 99ms/step - loss: 3.1313 - accuracy: 0.2302 - top-5-accuracy: 0.5226 - val_loss: 2.8741 - val_accuracy: 0.2804 - val_top-5-accuracy: 0.5790\n",
      "Epoch 7/70\n",
      "352/352 [==============================] - 35s 98ms/step - loss: 2.9954 - accuracy: 0.2549 - top-5-accuracy: 0.5572 - val_loss: 2.7578 - val_accuracy: 0.2986 - val_top-5-accuracy: 0.6140\n",
      "Epoch 8/70\n",
      "352/352 [==============================] - 35s 99ms/step - loss: 2.8795 - accuracy: 0.2794 - top-5-accuracy: 0.5865 - val_loss: 2.5976 - val_accuracy: 0.3394 - val_top-5-accuracy: 0.6488\n",
      "Epoch 9/70\n",
      "352/352 [==============================] - 36s 101ms/step - loss: 2.7704 - accuracy: 0.3018 - top-5-accuracy: 0.6112 - val_loss: 2.5403 - val_accuracy: 0.3520 - val_top-5-accuracy: 0.6668\n",
      "Epoch 10/70\n",
      "352/352 [==============================] - 35s 99ms/step - loss: 2.6722 - accuracy: 0.3218 - top-5-accuracy: 0.6342 - val_loss: 2.4570 - val_accuracy: 0.3776 - val_top-5-accuracy: 0.6706\n",
      "Epoch 11/70\n",
      "352/352 [==============================] - 35s 99ms/step - loss: 2.5928 - accuracy: 0.3368 - top-5-accuracy: 0.6530 - val_loss: 2.4055 - val_accuracy: 0.3794 - val_top-5-accuracy: 0.6876\n",
      "Epoch 12/70\n",
      "352/352 [==============================] - 35s 98ms/step - loss: 2.5037 - accuracy: 0.3551 - top-5-accuracy: 0.6727 - val_loss: 2.3461 - val_accuracy: 0.3930 - val_top-5-accuracy: 0.6998\n",
      "Epoch 13/70\n",
      "352/352 [==============================] - 35s 99ms/step - loss: 2.4376 - accuracy: 0.3734 - top-5-accuracy: 0.6850 - val_loss: 2.2972 - val_accuracy: 0.4072 - val_top-5-accuracy: 0.7070\n",
      "Epoch 14/70\n",
      "352/352 [==============================] - 35s 100ms/step - loss: 2.3719 - accuracy: 0.3840 - top-5-accuracy: 0.6980 - val_loss: 2.2499 - val_accuracy: 0.4122 - val_top-5-accuracy: 0.7160\n",
      "Epoch 15/70\n",
      "352/352 [==============================] - 35s 100ms/step - loss: 2.3123 - accuracy: 0.3956 - top-5-accuracy: 0.7135 - val_loss: 2.2345 - val_accuracy: 0.4126 - val_top-5-accuracy: 0.7188\n",
      "Epoch 16/70\n",
      "352/352 [==============================] - 35s 99ms/step - loss: 2.2625 - accuracy: 0.4070 - top-5-accuracy: 0.7248 - val_loss: 2.2091 - val_accuracy: 0.4254 - val_top-5-accuracy: 0.7252\n",
      "Epoch 17/70\n",
      "352/352 [==============================] - 35s 99ms/step - loss: 2.2021 - accuracy: 0.4194 - top-5-accuracy: 0.7359 - val_loss: 2.1604 - val_accuracy: 0.4356 - val_top-5-accuracy: 0.7320\n",
      "Epoch 18/70\n",
      "352/352 [==============================] - 34s 98ms/step - loss: 2.1627 - accuracy: 0.4283 - top-5-accuracy: 0.7441 - val_loss: 2.1861 - val_accuracy: 0.4372 - val_top-5-accuracy: 0.7274\n",
      "Epoch 19/70\n",
      "352/352 [==============================] - 35s 99ms/step - loss: 2.1102 - accuracy: 0.4388 - top-5-accuracy: 0.7545 - val_loss: 2.1122 - val_accuracy: 0.4418 - val_top-5-accuracy: 0.7434\n",
      "Epoch 20/70\n",
      "352/352 [==============================] - 35s 99ms/step - loss: 2.0697 - accuracy: 0.4494 - top-5-accuracy: 0.7614 - val_loss: 2.0751 - val_accuracy: 0.4538 - val_top-5-accuracy: 0.7494\n",
      "Epoch 21/70\n",
      "352/352 [==============================] - 35s 99ms/step - loss: 2.0275 - accuracy: 0.4590 - top-5-accuracy: 0.7700 - val_loss: 2.0537 - val_accuracy: 0.4672 - val_top-5-accuracy: 0.7548\n",
      "Epoch 22/70\n",
      "352/352 [==============================] - 34s 96ms/step - loss: 1.9871 - accuracy: 0.4662 - top-5-accuracy: 0.7779 - val_loss: 2.0911 - val_accuracy: 0.4514 - val_top-5-accuracy: 0.7496\n",
      "Epoch 23/70\n",
      "352/352 [==============================] - 35s 98ms/step - loss: 1.9536 - accuracy: 0.4732 - top-5-accuracy: 0.7832 - val_loss: 2.0195 - val_accuracy: 0.4684 - val_top-5-accuracy: 0.7620\n",
      "Epoch 24/70\n",
      "352/352 [==============================] - 34s 98ms/step - loss: 1.9266 - accuracy: 0.4805 - top-5-accuracy: 0.7884 - val_loss: 2.0265 - val_accuracy: 0.4704 - val_top-5-accuracy: 0.7636\n",
      "Epoch 25/70\n",
      "352/352 [==============================] - 34s 96ms/step - loss: 1.8806 - accuracy: 0.4914 - top-5-accuracy: 0.7959 - val_loss: 2.0186 - val_accuracy: 0.4684 - val_top-5-accuracy: 0.7662\n",
      "Epoch 26/70\n",
      "352/352 [==============================] - 34s 97ms/step - loss: 1.8507 - accuracy: 0.4977 - top-5-accuracy: 0.8027 - val_loss: 1.9725 - val_accuracy: 0.4828 - val_top-5-accuracy: 0.7730\n",
      "Epoch 27/70\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 1.8241 - accuracy: 0.5008 - top-5-accuracy: 0.8065 - val_loss: 1.9823 - val_accuracy: 0.4748 - val_top-5-accuracy: 0.7700\n",
      "Epoch 28/70\n",
      "352/352 [==============================] - 34s 97ms/step - loss: 1.7994 - accuracy: 0.5099 - top-5-accuracy: 0.8103 - val_loss: 1.9680 - val_accuracy: 0.4840 - val_top-5-accuracy: 0.7714\n",
      "Epoch 29/70\n",
      "352/352 [==============================] - 31s 87ms/step - loss: 1.7684 - accuracy: 0.5168 - top-5-accuracy: 0.8162 - val_loss: 1.9927 - val_accuracy: 0.4698 - val_top-5-accuracy: 0.7756\n",
      "Epoch 30/70\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 1.7481 - accuracy: 0.5212 - top-5-accuracy: 0.8216 - val_loss: 1.9570 - val_accuracy: 0.4820 - val_top-5-accuracy: 0.7832\n",
      "Epoch 31/70\n",
      "352/352 [==============================] - 33s 93ms/step - loss: 1.7191 - accuracy: 0.5294 - top-5-accuracy: 0.8257 - val_loss: 1.9618 - val_accuracy: 0.4784 - val_top-5-accuracy: 0.7764\n",
      "Epoch 32/70\n",
      "352/352 [==============================] - 34s 96ms/step - loss: 1.6917 - accuracy: 0.5340 - top-5-accuracy: 0.8307 - val_loss: 1.9597 - val_accuracy: 0.4892 - val_top-5-accuracy: 0.7740\n",
      "Epoch 33/70\n",
      "352/352 [==============================] - 31s 88ms/step - loss: 1.6591 - accuracy: 0.5421 - top-5-accuracy: 0.8365 - val_loss: 1.9577 - val_accuracy: 0.4944 - val_top-5-accuracy: 0.7794\n",
      "Epoch 34/70\n",
      "352/352 [==============================] - 29s 84ms/step - loss: 1.6397 - accuracy: 0.5474 - top-5-accuracy: 0.8405 - val_loss: 1.9520 - val_accuracy: 0.4880 - val_top-5-accuracy: 0.7830\n",
      "Epoch 35/70\n",
      "352/352 [==============================] - 31s 89ms/step - loss: 1.6080 - accuracy: 0.5544 - top-5-accuracy: 0.8438 - val_loss: 1.9073 - val_accuracy: 0.4998 - val_top-5-accuracy: 0.7912\n",
      "Epoch 36/70\n",
      "352/352 [==============================] - 30s 86ms/step - loss: 1.5909 - accuracy: 0.5580 - top-5-accuracy: 0.8490 - val_loss: 1.9367 - val_accuracy: 0.5010 - val_top-5-accuracy: 0.7900\n",
      "Epoch 37/70\n",
      "352/352 [==============================] - 30s 84ms/step - loss: 1.5648 - accuracy: 0.5635 - top-5-accuracy: 0.8521 - val_loss: 1.9192 - val_accuracy: 0.4938 - val_top-5-accuracy: 0.7816\n",
      "Epoch 38/70\n",
      "352/352 [==============================] - 31s 89ms/step - loss: 1.5421 - accuracy: 0.5689 - top-5-accuracy: 0.8577 - val_loss: 1.9077 - val_accuracy: 0.5102 - val_top-5-accuracy: 0.7876\n",
      "Epoch 39/70\n",
      "352/352 [==============================] - 30s 85ms/step - loss: 1.5152 - accuracy: 0.5764 - top-5-accuracy: 0.8624 - val_loss: 1.8758 - val_accuracy: 0.5102 - val_top-5-accuracy: 0.7966\n",
      "Epoch 40/70\n",
      "352/352 [==============================] - 31s 88ms/step - loss: 1.5073 - accuracy: 0.5779 - top-5-accuracy: 0.8627 - val_loss: 1.8901 - val_accuracy: 0.5144 - val_top-5-accuracy: 0.7948\n",
      "Epoch 41/70\n",
      "352/352 [==============================] - 30s 84ms/step - loss: 1.4783 - accuracy: 0.5829 - top-5-accuracy: 0.8690 - val_loss: 1.9214 - val_accuracy: 0.5092 - val_top-5-accuracy: 0.7936\n",
      "Epoch 42/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 31s 87ms/step - loss: 1.4739 - accuracy: 0.5880 - top-5-accuracy: 0.8682 - val_loss: 1.8629 - val_accuracy: 0.5086 - val_top-5-accuracy: 0.7966\n",
      "Epoch 43/70\n",
      "352/352 [==============================] - 31s 89ms/step - loss: 1.4462 - accuracy: 0.5931 - top-5-accuracy: 0.8736 - val_loss: 1.8921 - val_accuracy: 0.5148 - val_top-5-accuracy: 0.7952\n",
      "Epoch 44/70\n",
      "352/352 [==============================] - 30s 85ms/step - loss: 1.4299 - accuracy: 0.5936 - top-5-accuracy: 0.8745 - val_loss: 1.9003 - val_accuracy: 0.5078 - val_top-5-accuracy: 0.7920\n",
      "Epoch 45/70\n",
      "352/352 [==============================] - 31s 88ms/step - loss: 1.4119 - accuracy: 0.6009 - top-5-accuracy: 0.8784 - val_loss: 1.8705 - val_accuracy: 0.5230 - val_top-5-accuracy: 0.7948\n",
      "Epoch 46/70\n",
      "352/352 [==============================] - 30s 85ms/step - loss: 1.3896 - accuracy: 0.6060 - top-5-accuracy: 0.8813 - val_loss: 1.8769 - val_accuracy: 0.5190 - val_top-5-accuracy: 0.8092\n",
      "Epoch 47/70\n",
      "352/352 [==============================] - 31s 88ms/step - loss: 1.3854 - accuracy: 0.6036 - top-5-accuracy: 0.8819 - val_loss: 1.8499 - val_accuracy: 0.5264 - val_top-5-accuracy: 0.8038\n",
      "Epoch 48/70\n",
      "352/352 [==============================] - 29s 84ms/step - loss: 1.3640 - accuracy: 0.6165 - top-5-accuracy: 0.8856 - val_loss: 1.8571 - val_accuracy: 0.5192 - val_top-5-accuracy: 0.8008\n",
      "Epoch 49/70\n",
      "352/352 [==============================] - 30s 86ms/step - loss: 1.3588 - accuracy: 0.6144 - top-5-accuracy: 0.8864 - val_loss: 1.8701 - val_accuracy: 0.5148 - val_top-5-accuracy: 0.7998\n",
      "Epoch 50/70\n",
      "352/352 [==============================] - 30s 86ms/step - loss: 1.3365 - accuracy: 0.6196 - top-5-accuracy: 0.8894 - val_loss: 1.8830 - val_accuracy: 0.5116 - val_top-5-accuracy: 0.7972\n",
      "Epoch 51/70\n",
      "352/352 [==============================] - 30s 85ms/step - loss: 1.3329 - accuracy: 0.6193 - top-5-accuracy: 0.8918 - val_loss: 1.8901 - val_accuracy: 0.5126 - val_top-5-accuracy: 0.7958\n",
      "Epoch 52/70\n",
      "352/352 [==============================] - 30s 84ms/step - loss: 1.3132 - accuracy: 0.6255 - top-5-accuracy: 0.8940 - val_loss: 1.8346 - val_accuracy: 0.5230 - val_top-5-accuracy: 0.7976\n",
      "Epoch 53/70\n",
      "352/352 [==============================] - 29s 81ms/step - loss: 1.3168 - accuracy: 0.6260 - top-5-accuracy: 0.8934 - val_loss: 1.8544 - val_accuracy: 0.5232 - val_top-5-accuracy: 0.8044\n",
      "Epoch 54/70\n",
      "352/352 [==============================] - 28s 81ms/step - loss: 1.3123 - accuracy: 0.6266 - top-5-accuracy: 0.8950 - val_loss: 1.8673 - val_accuracy: 0.5198 - val_top-5-accuracy: 0.8008\n",
      "Epoch 55/70\n",
      "352/352 [==============================] - 28s 81ms/step - loss: 1.3037 - accuracy: 0.6272 - top-5-accuracy: 0.8965 - val_loss: 1.8611 - val_accuracy: 0.5250 - val_top-5-accuracy: 0.7998\n",
      "Epoch 56/70\n",
      "352/352 [==============================] - 28s 81ms/step - loss: 1.2776 - accuracy: 0.6330 - top-5-accuracy: 0.8980 - val_loss: 1.8982 - val_accuracy: 0.5130 - val_top-5-accuracy: 0.7986\n",
      "Epoch 57/70\n",
      "352/352 [==============================] - 29s 82ms/step - loss: 1.2776 - accuracy: 0.6330 - top-5-accuracy: 0.9005 - val_loss: 1.8374 - val_accuracy: 0.5298 - val_top-5-accuracy: 0.8070\n",
      "Epoch 58/70\n",
      "352/352 [==============================] - 28s 81ms/step - loss: 1.2512 - accuracy: 0.6429 - top-5-accuracy: 0.9022 - val_loss: 1.8454 - val_accuracy: 0.5266 - val_top-5-accuracy: 0.8034\n",
      "Epoch 59/70\n",
      "352/352 [==============================] - 28s 81ms/step - loss: 1.2503 - accuracy: 0.6396 - top-5-accuracy: 0.9036 - val_loss: 1.8429 - val_accuracy: 0.5276 - val_top-5-accuracy: 0.8022\n",
      "Epoch 60/70\n",
      "352/352 [==============================] - 28s 81ms/step - loss: 1.2408 - accuracy: 0.6411 - top-5-accuracy: 0.9063 - val_loss: 1.8792 - val_accuracy: 0.5186 - val_top-5-accuracy: 0.8000\n",
      "Epoch 61/70\n",
      "352/352 [==============================] - 28s 81ms/step - loss: 1.2450 - accuracy: 0.6447 - top-5-accuracy: 0.9043 - val_loss: 1.8568 - val_accuracy: 0.5216 - val_top-5-accuracy: 0.8038\n",
      "Epoch 62/70\n",
      "352/352 [==============================] - 29s 83ms/step - loss: 1.2193 - accuracy: 0.6475 - top-5-accuracy: 0.9092 - val_loss: 1.8497 - val_accuracy: 0.5306 - val_top-5-accuracy: 0.8098\n",
      "Epoch 63/70\n",
      "352/352 [==============================] - 28s 80ms/step - loss: 1.2144 - accuracy: 0.6497 - top-5-accuracy: 0.9082 - val_loss: 1.9119 - val_accuracy: 0.5144 - val_top-5-accuracy: 0.7952\n",
      "Epoch 64/70\n",
      "352/352 [==============================] - 28s 80ms/step - loss: 1.2155 - accuracy: 0.6486 - top-5-accuracy: 0.9098 - val_loss: 1.8980 - val_accuracy: 0.5236 - val_top-5-accuracy: 0.8064\n",
      "Epoch 65/70\n",
      "352/352 [==============================] - 28s 80ms/step - loss: 1.2078 - accuracy: 0.6513 - top-5-accuracy: 0.9088 - val_loss: 1.8712 - val_accuracy: 0.5240 - val_top-5-accuracy: 0.7960\n",
      "Epoch 66/70\n",
      "352/352 [==============================] - 28s 80ms/step - loss: 1.1921 - accuracy: 0.6591 - top-5-accuracy: 0.9122 - val_loss: 1.8695 - val_accuracy: 0.5258 - val_top-5-accuracy: 0.7978\n",
      "Epoch 67/70\n",
      "352/352 [==============================] - 28s 81ms/step - loss: 1.1944 - accuracy: 0.6544 - top-5-accuracy: 0.9128 - val_loss: 1.8731 - val_accuracy: 0.5326 - val_top-5-accuracy: 0.8088\n",
      "Epoch 68/70\n",
      "352/352 [==============================] - 27s 78ms/step - loss: 1.1721 - accuracy: 0.6599 - top-5-accuracy: 0.9142 - val_loss: 1.8630 - val_accuracy: 0.5288 - val_top-5-accuracy: 0.8002\n",
      "Epoch 69/70\n",
      "352/352 [==============================] - 28s 78ms/step - loss: 1.1727 - accuracy: 0.6607 - top-5-accuracy: 0.9135 - val_loss: 1.8851 - val_accuracy: 0.5242 - val_top-5-accuracy: 0.8074\n",
      "Epoch 70/70\n",
      "352/352 [==============================] - 27s 78ms/step - loss: 1.1666 - accuracy: 0.6626 - top-5-accuracy: 0.9148 - val_loss: 1.8493 - val_accuracy: 0.5272 - val_top-5-accuracy: 0.8132\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 1.8383 - accuracy: 0.5274 - top-5-accuracy: 0.8090\n",
      "Test accuracy: 52.74%\n",
      "Test top 5 accuracy: 80.9%\n",
      "CPU times: user 34min 57s, sys: 4min 12s, total: 39min 9s\n",
      "Wall time: 37min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy: 52.74%\n",
    "Test top 5 accuracy: 80.9%\n",
    "CPU times: user 34min 57s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'top-5-accuracy', 'val_loss', 'val_accuracy', 'val_top-5-accuracy'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [4.449682712554932,\n",
       "  3.9233672618865967,\n",
       "  3.6713178157806396,\n",
       "  3.4657657146453857,\n",
       "  3.2824037075042725,\n",
       "  3.131308078765869,\n",
       "  2.9954068660736084,\n",
       "  2.8794708251953125,\n",
       "  2.7703630924224854,\n",
       "  2.672229766845703,\n",
       "  2.59281849861145,\n",
       "  2.5036535263061523,\n",
       "  2.437554121017456,\n",
       "  2.3719100952148438,\n",
       "  2.3123044967651367,\n",
       "  2.262467861175537,\n",
       "  2.202073097229004,\n",
       "  2.162694215774536,\n",
       "  2.110203981399536,\n",
       "  2.069697618484497,\n",
       "  2.02748441696167,\n",
       "  1.987126350402832,\n",
       "  1.9535781145095825,\n",
       "  1.9266091585159302,\n",
       "  1.880560278892517,\n",
       "  1.850748062133789,\n",
       "  1.8240970373153687,\n",
       "  1.7994352579116821,\n",
       "  1.7684409618377686,\n",
       "  1.7480807304382324,\n",
       "  1.7190998792648315,\n",
       "  1.6917014122009277,\n",
       "  1.6590588092803955,\n",
       "  1.6396647691726685,\n",
       "  1.60804283618927,\n",
       "  1.5908782482147217,\n",
       "  1.564848780632019,\n",
       "  1.542140245437622,\n",
       "  1.5152376890182495,\n",
       "  1.5073015689849854,\n",
       "  1.4783093929290771,\n",
       "  1.4738996028900146,\n",
       "  1.4462454319000244,\n",
       "  1.4299023151397705,\n",
       "  1.4118664264678955,\n",
       "  1.3896247148513794,\n",
       "  1.3854206800460815,\n",
       "  1.3639836311340332,\n",
       "  1.3588207960128784,\n",
       "  1.3364595174789429,\n",
       "  1.3328841924667358,\n",
       "  1.3132107257843018,\n",
       "  1.316773772239685,\n",
       "  1.3122613430023193,\n",
       "  1.3036638498306274,\n",
       "  1.2776445150375366,\n",
       "  1.2775814533233643,\n",
       "  1.251150369644165,\n",
       "  1.2503013610839844,\n",
       "  1.2408077716827393,\n",
       "  1.2450259923934937,\n",
       "  1.2192811965942383,\n",
       "  1.2143898010253906,\n",
       "  1.2154806852340698,\n",
       "  1.2077593803405762,\n",
       "  1.192128300666809,\n",
       "  1.1943966150283813,\n",
       "  1.1721477508544922,\n",
       "  1.1727159023284912,\n",
       "  1.1666327714920044],\n",
       " 'accuracy': [0.04479999840259552,\n",
       "  0.09415555745363235,\n",
       "  0.1313111037015915,\n",
       "  0.16715554893016815,\n",
       "  0.20011110603809357,\n",
       "  0.23022222518920898,\n",
       "  0.25486665964126587,\n",
       "  0.2793777883052826,\n",
       "  0.30175554752349854,\n",
       "  0.32179999351501465,\n",
       "  0.33684444427490234,\n",
       "  0.3550666570663452,\n",
       "  0.37344443798065186,\n",
       "  0.38404443860054016,\n",
       "  0.3955777883529663,\n",
       "  0.4069555699825287,\n",
       "  0.4193555414676666,\n",
       "  0.42828887701034546,\n",
       "  0.43877777457237244,\n",
       "  0.44937777519226074,\n",
       "  0.4589777886867523,\n",
       "  0.4661777913570404,\n",
       "  0.47324445843696594,\n",
       "  0.48046666383743286,\n",
       "  0.4913555681705475,\n",
       "  0.497688889503479,\n",
       "  0.5008000135421753,\n",
       "  0.5098666548728943,\n",
       "  0.5168444514274597,\n",
       "  0.5212222337722778,\n",
       "  0.529355525970459,\n",
       "  0.5339778065681458,\n",
       "  0.5420666933059692,\n",
       "  0.5473999977111816,\n",
       "  0.5544221997261047,\n",
       "  0.5579555630683899,\n",
       "  0.5635333061218262,\n",
       "  0.5688889026641846,\n",
       "  0.5763555765151978,\n",
       "  0.5779111385345459,\n",
       "  0.5829333066940308,\n",
       "  0.5879555344581604,\n",
       "  0.5930666923522949,\n",
       "  0.5935778021812439,\n",
       "  0.600933313369751,\n",
       "  0.6060444712638855,\n",
       "  0.6036221981048584,\n",
       "  0.6165333390235901,\n",
       "  0.6143777966499329,\n",
       "  0.6195555329322815,\n",
       "  0.6193333268165588,\n",
       "  0.6254666447639465,\n",
       "  0.626022219657898,\n",
       "  0.6265555620193481,\n",
       "  0.6272000074386597,\n",
       "  0.6330222487449646,\n",
       "  0.6329555511474609,\n",
       "  0.6428889036178589,\n",
       "  0.6395778059959412,\n",
       "  0.6410666704177856,\n",
       "  0.6446889042854309,\n",
       "  0.6474666595458984,\n",
       "  0.6497111320495605,\n",
       "  0.6485777497291565,\n",
       "  0.6513333320617676,\n",
       "  0.6590889096260071,\n",
       "  0.6544222235679626,\n",
       "  0.6599110960960388,\n",
       "  0.6606888771057129,\n",
       "  0.6625999808311462],\n",
       " 'top-5-accuracy': [0.16197778284549713,\n",
       "  0.29606667160987854,\n",
       "  0.3704666793346405,\n",
       "  0.4346666634082794,\n",
       "  0.48311111330986023,\n",
       "  0.522599995136261,\n",
       "  0.557200014591217,\n",
       "  0.5865333080291748,\n",
       "  0.6112444400787354,\n",
       "  0.6342222094535828,\n",
       "  0.6530444622039795,\n",
       "  0.6727333068847656,\n",
       "  0.6849777698516846,\n",
       "  0.6980444192886353,\n",
       "  0.7135111093521118,\n",
       "  0.7247777581214905,\n",
       "  0.7359111309051514,\n",
       "  0.7441111207008362,\n",
       "  0.7545111179351807,\n",
       "  0.7614444494247437,\n",
       "  0.7700444459915161,\n",
       "  0.7779333591461182,\n",
       "  0.7831777930259705,\n",
       "  0.7883999943733215,\n",
       "  0.7958889007568359,\n",
       "  0.8027111291885376,\n",
       "  0.8064888715744019,\n",
       "  0.8103111386299133,\n",
       "  0.8161777853965759,\n",
       "  0.8215555548667908,\n",
       "  0.8256666660308838,\n",
       "  0.8307333588600159,\n",
       "  0.8365333080291748,\n",
       "  0.8404889106750488,\n",
       "  0.8437777757644653,\n",
       "  0.8489999771118164,\n",
       "  0.8521111011505127,\n",
       "  0.8577333092689514,\n",
       "  0.8623999953269958,\n",
       "  0.8627333045005798,\n",
       "  0.8689777851104736,\n",
       "  0.868244469165802,\n",
       "  0.8736444711685181,\n",
       "  0.8744666576385498,\n",
       "  0.8784444332122803,\n",
       "  0.8813333511352539,\n",
       "  0.8818666934967041,\n",
       "  0.8856444358825684,\n",
       "  0.8864444494247437,\n",
       "  0.8893555402755737,\n",
       "  0.8918444514274597,\n",
       "  0.8939999938011169,\n",
       "  0.8933555483818054,\n",
       "  0.8949777483940125,\n",
       "  0.8965111374855042,\n",
       "  0.8980000019073486,\n",
       "  0.9004889130592346,\n",
       "  0.9021555781364441,\n",
       "  0.9035999774932861,\n",
       "  0.906333327293396,\n",
       "  0.9042666554450989,\n",
       "  0.9092444181442261,\n",
       "  0.9081555604934692,\n",
       "  0.909755527973175,\n",
       "  0.9087777733802795,\n",
       "  0.9121999740600586,\n",
       "  0.9128000140190125,\n",
       "  0.9142444729804993,\n",
       "  0.9134666919708252,\n",
       "  0.9147777557373047],\n",
       " 'val_loss': [3.9295811653137207,\n",
       "  3.564678907394409,\n",
       "  3.287400484085083,\n",
       "  3.1223742961883545,\n",
       "  2.9786365032196045,\n",
       "  2.8741424083709717,\n",
       "  2.7578067779541016,\n",
       "  2.597639560699463,\n",
       "  2.5402891635894775,\n",
       "  2.4569554328918457,\n",
       "  2.40549373626709,\n",
       "  2.346115827560425,\n",
       "  2.2972252368927,\n",
       "  2.2499146461486816,\n",
       "  2.234531879425049,\n",
       "  2.209141969680786,\n",
       "  2.160379409790039,\n",
       "  2.186117172241211,\n",
       "  2.1122078895568848,\n",
       "  2.075082778930664,\n",
       "  2.0536861419677734,\n",
       "  2.0910580158233643,\n",
       "  2.019484043121338,\n",
       "  2.026500701904297,\n",
       "  2.0186283588409424,\n",
       "  1.9725474119186401,\n",
       "  1.9823132753372192,\n",
       "  1.9679521322250366,\n",
       "  1.992701530456543,\n",
       "  1.9569737911224365,\n",
       "  1.9618293046951294,\n",
       "  1.9597039222717285,\n",
       "  1.957716941833496,\n",
       "  1.9519532918930054,\n",
       "  1.9072786569595337,\n",
       "  1.9366501569747925,\n",
       "  1.9192090034484863,\n",
       "  1.9076627492904663,\n",
       "  1.875780463218689,\n",
       "  1.89005708694458,\n",
       "  1.92135488986969,\n",
       "  1.8629498481750488,\n",
       "  1.8920992612838745,\n",
       "  1.9003044366836548,\n",
       "  1.8705053329467773,\n",
       "  1.8768656253814697,\n",
       "  1.8499343395233154,\n",
       "  1.857089638710022,\n",
       "  1.8700759410858154,\n",
       "  1.8830190896987915,\n",
       "  1.8901082277297974,\n",
       "  1.8345555067062378,\n",
       "  1.854410171508789,\n",
       "  1.867315411567688,\n",
       "  1.8610949516296387,\n",
       "  1.8982408046722412,\n",
       "  1.8373637199401855,\n",
       "  1.8454241752624512,\n",
       "  1.8429125547409058,\n",
       "  1.879230260848999,\n",
       "  1.8568472862243652,\n",
       "  1.8496851921081543,\n",
       "  1.911867618560791,\n",
       "  1.8980199098587036,\n",
       "  1.8711928129196167,\n",
       "  1.8695226907730103,\n",
       "  1.8730801343917847,\n",
       "  1.8630484342575073,\n",
       "  1.885147213935852,\n",
       "  1.849331021308899],\n",
       " 'val_accuracy': [0.09380000084638596,\n",
       "  0.1469999998807907,\n",
       "  0.20900000631809235,\n",
       "  0.2264000028371811,\n",
       "  0.25459998846054077,\n",
       "  0.28040000796318054,\n",
       "  0.2985999882221222,\n",
       "  0.3393999934196472,\n",
       "  0.35199999809265137,\n",
       "  0.3776000142097473,\n",
       "  0.37940001487731934,\n",
       "  0.3930000066757202,\n",
       "  0.40720000863075256,\n",
       "  0.412200003862381,\n",
       "  0.41260001063346863,\n",
       "  0.4253999888896942,\n",
       "  0.43560001254081726,\n",
       "  0.43720000982284546,\n",
       "  0.44179999828338623,\n",
       "  0.4537999927997589,\n",
       "  0.46720001101493835,\n",
       "  0.4514000117778778,\n",
       "  0.4684000015258789,\n",
       "  0.47040000557899475,\n",
       "  0.4684000015258789,\n",
       "  0.4828000068664551,\n",
       "  0.4747999906539917,\n",
       "  0.48399999737739563,\n",
       "  0.4697999954223633,\n",
       "  0.4819999933242798,\n",
       "  0.47839999198913574,\n",
       "  0.48919999599456787,\n",
       "  0.4943999946117401,\n",
       "  0.4880000054836273,\n",
       "  0.4997999966144562,\n",
       "  0.5009999871253967,\n",
       "  0.49380001425743103,\n",
       "  0.510200023651123,\n",
       "  0.510200023651123,\n",
       "  0.5144000053405762,\n",
       "  0.5091999769210815,\n",
       "  0.5085999965667725,\n",
       "  0.5148000121116638,\n",
       "  0.5077999830245972,\n",
       "  0.5230000019073486,\n",
       "  0.5189999938011169,\n",
       "  0.5264000296592712,\n",
       "  0.5192000269889832,\n",
       "  0.5148000121116638,\n",
       "  0.5116000175476074,\n",
       "  0.5126000046730042,\n",
       "  0.5230000019073486,\n",
       "  0.5231999754905701,\n",
       "  0.5198000073432922,\n",
       "  0.5249999761581421,\n",
       "  0.5130000114440918,\n",
       "  0.5297999978065491,\n",
       "  0.5266000032424927,\n",
       "  0.5275999903678894,\n",
       "  0.5185999870300293,\n",
       "  0.5216000080108643,\n",
       "  0.5306000113487244,\n",
       "  0.5144000053405762,\n",
       "  0.5235999822616577,\n",
       "  0.5239999890327454,\n",
       "  0.5257999897003174,\n",
       "  0.5325999855995178,\n",
       "  0.5288000106811523,\n",
       "  0.5242000222206116,\n",
       "  0.5271999835968018],\n",
       " 'val_top-5-accuracy': [0.2996000051498413,\n",
       "  0.4106000065803528,\n",
       "  0.4832000136375427,\n",
       "  0.5216000080108643,\n",
       "  0.5576000213623047,\n",
       "  0.5789999961853027,\n",
       "  0.6140000224113464,\n",
       "  0.6488000154495239,\n",
       "  0.6668000221252441,\n",
       "  0.6705999970436096,\n",
       "  0.6876000165939331,\n",
       "  0.6998000144958496,\n",
       "  0.7070000171661377,\n",
       "  0.7160000205039978,\n",
       "  0.7188000082969666,\n",
       "  0.7251999974250793,\n",
       "  0.7319999933242798,\n",
       "  0.727400004863739,\n",
       "  0.743399977684021,\n",
       "  0.7494000196456909,\n",
       "  0.754800021648407,\n",
       "  0.7495999932289124,\n",
       "  0.7620000243186951,\n",
       "  0.7635999917984009,\n",
       "  0.7662000060081482,\n",
       "  0.7730000019073486,\n",
       "  0.7699999809265137,\n",
       "  0.771399974822998,\n",
       "  0.775600016117096,\n",
       "  0.7832000255584717,\n",
       "  0.7764000296592712,\n",
       "  0.7739999890327454,\n",
       "  0.7793999910354614,\n",
       "  0.7829999923706055,\n",
       "  0.7911999821662903,\n",
       "  0.7900000214576721,\n",
       "  0.7815999984741211,\n",
       "  0.7875999808311462,\n",
       "  0.7965999841690063,\n",
       "  0.7947999835014343,\n",
       "  0.7936000227928162,\n",
       "  0.7965999841690063,\n",
       "  0.795199990272522,\n",
       "  0.7919999957084656,\n",
       "  0.7947999835014343,\n",
       "  0.8091999888420105,\n",
       "  0.8037999868392944,\n",
       "  0.8008000254631042,\n",
       "  0.7997999787330627,\n",
       "  0.7972000241279602,\n",
       "  0.795799970626831,\n",
       "  0.7975999712944031,\n",
       "  0.8044000267982483,\n",
       "  0.8008000254631042,\n",
       "  0.7997999787330627,\n",
       "  0.7986000180244446,\n",
       "  0.8069999814033508,\n",
       "  0.8033999800682068,\n",
       "  0.8022000193595886,\n",
       "  0.800000011920929,\n",
       "  0.8037999868392944,\n",
       "  0.8098000288009644,\n",
       "  0.795199990272522,\n",
       "  0.8064000010490417,\n",
       "  0.7960000038146973,\n",
       "  0.7978000044822693,\n",
       "  0.8087999820709229,\n",
       "  0.8001999855041504,\n",
       "  0.8073999881744385,\n",
       "  0.8131999969482422]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4XQi3l4lF25"
   },
   "source": [
    "After 100 epochs, the ViT model achieves around 55% accuracy and\n",
    "82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n",
    "as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n",
    "\n",
    "Note that the state of the art results reported in the\n",
    "[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n",
    "the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n",
    "without pre-training, you can try to train the model for more epochs, use a larger number of\n",
    "Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. \n",
    "Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, \n",
    "but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n",
    "In practice, it's recommended to fine-tune a ViT model\n",
    "that was pre-trained using a large, high-resolution dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "image_classification_with_vision_transformer",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
