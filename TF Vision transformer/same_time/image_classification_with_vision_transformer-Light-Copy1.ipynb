{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhSmR2sClF2q"
   },
   "source": [
    "# Image classification with Vision Transformer\n",
    "\n",
    "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
    "**Date created:** 2021/01/18<br>\n",
    "**Last modified:** 2021/01/18<br>\n",
    "**Description:** Implementing the Vision Transformer (ViT) model for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdRUkrzIlF2s"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n",
    "model by Alexey Dosovitskiy et al. for image classification,\n",
    "and demonstrates it on the CIFAR-100 dataset.\n",
    "The ViT model applies the Transformer architecture with self-attention to sequences of\n",
    "image patches, without using convolution layers.\n",
    "\n",
    "This example requires TensorFlow 2.4 or higher, as well as\n",
    "[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n",
    "which can be installed using the following command:\n",
    "\n",
    "```python\n",
    "pip install -U tensorflow-addons\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4t7PrBOglF2u"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "saU-TVKRlF2v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pa7__gaulF2v"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxrdXFqMlF2w",
    "outputId": "efaf4923-61a0-4c4d-da86-ce6602be22b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1KSPGeRlF2x"
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hGLdPi98lF2y"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 82\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6nS03VTlF2y"
   },
   "source": [
    "## Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Cxqw5LRclF2z"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHlow8ValF20"
   },
   "source": [
    "## Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "U0DOttK1lF20"
   },
   "outputs": [],
   "source": [
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIYt9MLLlF21"
   },
   "source": [
    "## Implement patch creation as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qFsGYVg2lF22"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRhY_wWhlF22"
   },
   "source": [
    "Let's display patches for a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "gjZ5_CtTlF22",
    "outputId": "47af9281-7327-44cc-991d-20c6445d2383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 72 X 72\n",
      "Patch size: 6 X 6\n",
      "Patches per image: 144\n",
      "Elements per patch: 108\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 144 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ds_6wxWflF23"
   },
   "source": [
    "## Implement the patch encoding layer\n",
    "\n",
    "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
    "vector of size `projection_dim`. In addition, it adds a learnable position\n",
    "embedding to the projected vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L20K-tyHlF23"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vssO606QlF24"
   },
   "source": [
    "## Build the ViT model\n",
    "\n",
    "The ViT model consists of multiple Transformer blocks,\n",
    "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
    "applied to the sequence of patches. The Transformer blocks produce a\n",
    "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
    "classifier head with softmax to produce the final class probabilities output.\n",
    "\n",
    "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
    "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
    "as the image representation, all the outputs of the final Transformer block are\n",
    "reshaped with `layers.Flatten()` and used as the image\n",
    "representation input to the classifier head.\n",
    "Note that the `layers.GlobalAveragePooling1D` layer\n",
    "could also be used instead to aggregate the outputs of the Transformer block,\n",
    "especially when the number of patches and the projection dimensions are large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eZzsK1RPndDs"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Lint as: python3\n",
    "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Keras-based attention layer.\"\"\"\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "# pylint: disable=g-classes-have-attributes\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from keras import constraints\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.engine.base_layer import Layer\n",
    "from keras.layers import advanced_activations\n",
    "from keras.layers import core\n",
    "from keras.layers import einsum_dense\n",
    "from keras.utils import tf_utils\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "\n",
    "def _build_attention_equation(rank, attn_axes):\n",
    "  \"\"\"Builds einsum equations for the attention computation.\n",
    "\n",
    "  Query, key, value inputs after projection are expected to have the shape as:\n",
    "  `(bs, <non-attention dims>, <attention dims>, num_heads, channels)`.\n",
    "  `bs` and `<non-attention dims>` are treated as `<batch dims>`.\n",
    "\n",
    "  The attention operations can be generalized:\n",
    "  (1) Query-key dot product:\n",
    "  `(<batch dims>, <query attention dims>, num_heads, channels), (<batch dims>,\n",
    "  <key attention dims>, num_heads, channels) -> (<batch dims>,\n",
    "  num_heads, <query attention dims>, <key attention dims>)`\n",
    "  (2) Combination:\n",
    "  `(<batch dims>, num_heads, <query attention dims>, <key attention dims>),\n",
    "  (<batch dims>, <value attention dims>, num_heads, channels) -> (<batch dims>,\n",
    "  <query attention dims>, num_heads, channels)`\n",
    "\n",
    "  Args:\n",
    "    rank: Rank of query, key, value tensors.\n",
    "    attn_axes: List/tuple of axes, `[-1, rank)`,\n",
    "      that attention will be applied to.\n",
    "\n",
    "  Returns:\n",
    "    Einsum equations.\n",
    "  \"\"\"\n",
    "  target_notation = _CHR_IDX[:rank]\n",
    "  # `batch_dims` includes the head dim.\n",
    "  batch_dims = tuple(np.delete(range(rank), attn_axes + (rank - 1,)))\n",
    "  letter_offset = rank\n",
    "  source_notation = \"\"\n",
    "  for i in range(rank):\n",
    "    if i in batch_dims or i == rank - 1:\n",
    "      source_notation += target_notation[i]\n",
    "    else:\n",
    "      source_notation += _CHR_IDX[letter_offset]\n",
    "      letter_offset += 1\n",
    "\n",
    "  product_notation = \"\".join([target_notation[i] for i in batch_dims] +\n",
    "                             [target_notation[i] for i in attn_axes] +\n",
    "                             [source_notation[i] for i in attn_axes])\n",
    "  dot_product_equation = \"%s,%s->%s\" % (source_notation, target_notation,\n",
    "                                        product_notation)\n",
    "  attn_scores_rank = len(product_notation)\n",
    "  combine_equation = \"%s,%s->%s\" % (product_notation, source_notation,\n",
    "                                    target_notation)\n",
    "  return dot_product_equation, combine_equation, attn_scores_rank\n",
    "\n",
    "\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "  \"\"\"Builds an einsum equation for projections inside multi-head attention.\"\"\"\n",
    "  input_str = \"\"\n",
    "  kernel_str = \"\"\n",
    "  output_str = \"\"\n",
    "  bias_axes = \"\"\n",
    "  letter_offset = 0\n",
    "  for i in range(free_dims):\n",
    "    char = _CHR_IDX[i + letter_offset]\n",
    "    input_str += char\n",
    "    output_str += char\n",
    "\n",
    "  letter_offset += free_dims\n",
    "  for i in range(bound_dims):\n",
    "    char = _CHR_IDX[i + letter_offset]\n",
    "    input_str += char\n",
    "    kernel_str += char\n",
    "\n",
    "  letter_offset += bound_dims\n",
    "  for i in range(output_dims):\n",
    "    char = _CHR_IDX[i + letter_offset]\n",
    "    kernel_str += char\n",
    "    output_str += char\n",
    "    bias_axes += char\n",
    "  equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "  return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "  return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)\n",
    "\n",
    "\n",
    "@keras_export(\"keras.layers.MultiHeadAttention\")\n",
    "class MultiHeadAttention(Layer):\n",
    "  \"\"\"MultiHeadAttention layer.\n",
    "\n",
    "  This is an implementation of multi-headed attention as described in the paper\n",
    "  \"Attention is all you Need\" (Vaswani et al., 2017).\n",
    "  If `query`, `key,` `value` are the same, then\n",
    "  this is self-attention. Each timestep in `query` attends to the\n",
    "  corresponding sequence in `key`, and returns a fixed-width vector.\n",
    "\n",
    "  This layer first projects `query`, `key` and `value`. These are\n",
    "  (effectively) a list of tensors of length `num_attention_heads`, where the\n",
    "  corresponding shapes are `(batch_size, <query dimensions>, key_dim)`,\n",
    "  `(batch_size, <key/value dimensions>, key_dim)`,\n",
    "  `(batch_size, <key/value dimensions>, value_dim)`.\n",
    "\n",
    "  Then, the query and key tensors are dot-producted and scaled. These are\n",
    "  softmaxed to obtain attention probabilities. The value tensors are then\n",
    "  interpolated by these probabilities, then concatenated back to a single\n",
    "  tensor.\n",
    "\n",
    "  Finally, the result tensor with the last dimension as value_dim can take an\n",
    "  linear projection and return.\n",
    "\n",
    "  When using MultiHeadAttention inside a custom Layer, the custom Layer must\n",
    "  implement `build()` and call MultiHeadAttention's `_build_from_signature()`.\n",
    "  This enables weights to be restored correctly when the model is loaded.\n",
    "  TODO(b/172609172): link to documentation about calling custom build functions\n",
    "  when used in a custom Layer.\n",
    "\n",
    "  Examples:\n",
    "\n",
    "  Performs 1D cross-attention over two sequence inputs with an attention mask.\n",
    "  Returns the additional attention weights over heads.\n",
    "\n",
    "  >>> layer = MultiHeadAttention(num_heads=2, key_dim=2)\n",
    "  >>> target = tf.keras.Input(shape=[8, 16])\n",
    "  >>> source = tf.keras.Input(shape=[4, 16])\n",
    "  >>> output_tensor, weights = layer(target, source,\n",
    "  ...                                return_attention_scores=True)\n",
    "  >>> print(output_tensor.shape)\n",
    "  (None, 8, 16)\n",
    "  >>> print(weights.shape)\n",
    "  (None, 2, 8, 4)\n",
    "\n",
    "  Performs 2D self-attention over a 5D input tensor on axes 2 and 3.\n",
    "\n",
    "  >>> layer = MultiHeadAttention(num_heads=2, key_dim=2, attention_axes=(2, 3))\n",
    "  >>> input_tensor = tf.keras.Input(shape=[5, 3, 4, 16])\n",
    "  >>> output_tensor = layer(input_tensor, input_tensor)\n",
    "  >>> print(output_tensor.shape)\n",
    "  (None, 5, 3, 4, 16)\n",
    "\n",
    "  Args:\n",
    "    num_heads: Number of attention heads.\n",
    "    key_dim: Size of each attention head for query and key.\n",
    "    value_dim: Size of each attention head for value.\n",
    "    dropout: Dropout probability.\n",
    "    use_bias: Boolean, whether the dense layers use bias vectors/matrices.\n",
    "    output_shape: The expected shape of an output tensor, besides the batch and\n",
    "      sequence dims. If not specified, projects back to the key feature dim.\n",
    "    attention_axes: axes over which the attention is applied. `None` means\n",
    "      attention over all axes, but batch, heads, and features.\n",
    "    kernel_initializer: Initializer for dense layer kernels.\n",
    "    bias_initializer: Initializer for dense layer biases.\n",
    "    kernel_regularizer: Regularizer for dense layer kernels.\n",
    "    bias_regularizer: Regularizer for dense layer biases.\n",
    "    activity_regularizer: Regularizer for dense layer activity.\n",
    "    kernel_constraint: Constraint for dense layer kernels.\n",
    "    bias_constraint: Constraint for dense layer kernels.\n",
    "\n",
    "  Call arguments:\n",
    "    query: Query `Tensor` of shape `(B, T, dim)`.\n",
    "    value: Value `Tensor` of shape `(B, S, dim)`.\n",
    "    key: Optional key `Tensor` of shape `(B, S, dim)`. If not given, will use\n",
    "      `value` for both `key` and `value`, which is the most common case.\n",
    "    attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
    "      attention to certain positions. The boolean mask specifies which query\n",
    "      elements can attend to which key elements, 1 indicates attention and 0\n",
    "      indicates no attention. Broadcasting can happen for the missing batch\n",
    "      dimensions and the head dimension.\n",
    "    return_attention_scores: A boolean to indicate whether the output should\n",
    "      be `(attention_output, attention_scores)` if `True`, or `attention_output`\n",
    "      if `False`. Defaults to `False`.\n",
    "    training: Python boolean indicating whether the layer should behave in\n",
    "      training mode (adding dropout) or in inference mode (no dropout).\n",
    "      Defaults to either using the training mode of the parent layer/model,\n",
    "      or False (inference) if there is no parent layer.\n",
    "\n",
    "  Returns:\n",
    "    attention_output: The result of the computation, of shape `(B, T, E)`,\n",
    "      where `T` is for target sequence shapes and `E` is the query input last\n",
    "      dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "      are project to the shape specified by `output_shape`.\n",
    "    attention_scores: [Optional] multi-head attention coefficients over\n",
    "      attention axes.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_heads,\n",
    "               key_dim,\n",
    "               value_dim=None,\n",
    "               dropout=0.0,\n",
    "               use_bias=True,\n",
    "               output_shape=None,\n",
    "               attention_axes=None,\n",
    "               kernel_initializer=\"glorot_uniform\",\n",
    "               bias_initializer=\"zeros\",\n",
    "               kernel_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               activity_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               **kwargs):\n",
    "    super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "    self._num_heads = num_heads\n",
    "    self._key_dim = key_dim\n",
    "#     self._value_dim = value_dim if value_dim else key_dim\n",
    "    self._dropout = dropout\n",
    "    self._use_bias = use_bias\n",
    "    self._output_shape = output_shape\n",
    "    self._kernel_initializer = initializers.get(kernel_initializer)\n",
    "    self._bias_initializer = initializers.get(bias_initializer)\n",
    "    self._kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "    self._bias_regularizer = regularizers.get(bias_regularizer)\n",
    "    self._activity_regularizer = regularizers.get(activity_regularizer)\n",
    "    self._kernel_constraint = constraints.get(kernel_constraint)\n",
    "    self._bias_constraint = constraints.get(bias_constraint)\n",
    "    if attention_axes is not None and not isinstance(attention_axes,\n",
    "                                                     collections.abc.Sized):\n",
    "      self._attention_axes = (attention_axes,)\n",
    "    else:\n",
    "      self._attention_axes = attention_axes\n",
    "    self._built_from_signature = False\n",
    "    self._query_shape, self._key_shape, self._value_shape = None, None, None\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        \"num_heads\": self._num_heads,\n",
    "        \"key_dim\": self._key_dim,\n",
    "#         \"value_dim\": self._value_dim,\n",
    "        \"dropout\": self._dropout,\n",
    "        \"use_bias\": self._use_bias,\n",
    "        \"output_shape\": self._output_shape,\n",
    "        \"attention_axes\": self._attention_axes,\n",
    "        \"kernel_initializer\":\n",
    "            initializers.serialize(self._kernel_initializer),\n",
    "        \"bias_initializer\":\n",
    "            initializers.serialize(self._bias_initializer),\n",
    "        \"kernel_regularizer\":\n",
    "            regularizers.serialize(self._kernel_regularizer),\n",
    "        \"bias_regularizer\":\n",
    "            regularizers.serialize(self._bias_regularizer),\n",
    "        \"activity_regularizer\":\n",
    "            regularizers.serialize(self._activity_regularizer),\n",
    "        \"kernel_constraint\":\n",
    "            constraints.serialize(self._kernel_constraint),\n",
    "        \"bias_constraint\":\n",
    "            constraints.serialize(self._bias_constraint),\n",
    "        \"query_shape\": self._query_shape,\n",
    "        \"key_shape\": self._key_shape,\n",
    "        \"value_shape\": self._value_shape,\n",
    "    }\n",
    "    base_config = super(MultiHeadAttention, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    # If the layer has a different build() function from the Keras default,\n",
    "    # we need to trigger the customized build to create weights.\n",
    "    query_shape = config.pop(\"query_shape\")\n",
    "    key_shape = config.pop(\"key_shape\")\n",
    "    value_shape = config.pop(\"value_shape\")\n",
    "    layer = cls(**config)\n",
    "    if None in [query_shape, key_shape, value_shape]:\n",
    "      logging.warning(\n",
    "          \"One of dimensions of the input shape is missing. It should have been\"\n",
    "          \" memorized when the layer was serialized. \"\n",
    "          \"%s is created without weights.\",\n",
    "          str(cls))\n",
    "    else:\n",
    "      layer._build_from_signature(query_shape, value_shape, key_shape)  # pylint: disable=protected-access\n",
    "    return layer\n",
    "\n",
    "  def _build_from_signature(self, query, value, key=None):\n",
    "    \"\"\"Builds layers and variables.\n",
    "\n",
    "    Once the method is called, self._built_from_signature will be set to True.\n",
    "\n",
    "    Args:\n",
    "      query: Query tensor or TensorShape.\n",
    "      value: Value tensor or TensorShape.\n",
    "      key: Key tensor or TensorShape.\n",
    "    \"\"\"\n",
    "    self._built_from_signature = True\n",
    "    if hasattr(query, \"shape\"):\n",
    "      self._query_shape = tf.TensorShape(query.shape)\n",
    "    else:\n",
    "      self._query_shape = tf.TensorShape(query)\n",
    "#     if hasattr(value, \"shape\"):\n",
    "#       self._value_shape = tf.TensorShape(value.shape)\n",
    "#     else:\n",
    "#       self._value_shape = tf.TensorShape(value)\n",
    "    if key is None:\n",
    "       self._key_shape = tf.TensorShape(value.shape)\n",
    "    elif hasattr(key, \"shape\"):\n",
    "      self._key_shape = tf.TensorShape(key.shape)\n",
    "    else:\n",
    "      self._key_shape = tf.TensorShape(key)\n",
    "\n",
    "    common_kwargs = dict(\n",
    "        kernel_initializer=self._kernel_initializer,\n",
    "        bias_initializer=self._bias_initializer,\n",
    "        kernel_regularizer=self._kernel_regularizer,\n",
    "        bias_regularizer=self._bias_regularizer,\n",
    "        activity_regularizer=self._activity_regularizer,\n",
    "        kernel_constraint=self._kernel_constraint,\n",
    "        bias_constraint=self._bias_constraint)\n",
    "    # Any setup work performed only once should happen in an `init_scope`\n",
    "    # to avoid creating symbolic Tensors that will later pollute any eager\n",
    "    # operations.\n",
    "    with tf_utils.maybe_init_scope(self):\n",
    "      free_dims = self._query_shape.rank - 1\n",
    "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "          free_dims, bound_dims=1, output_dims=2)\n",
    "      self._query_dense = einsum_dense.EinsumDense(\n",
    "          einsum_equation,\n",
    "          output_shape=_get_output_shape(output_rank - 1,\n",
    "                                         [self._num_heads, self._key_dim]),\n",
    "          bias_axes=bias_axes if self._use_bias else None,\n",
    "          name=\"query\",\n",
    "          **common_kwargs)\n",
    "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "          self._key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "      self._key_dense = einsum_dense.EinsumDense(\n",
    "          einsum_equation,\n",
    "          output_shape=_get_output_shape(output_rank - 1,\n",
    "                                         [self._num_heads, self._key_dim]),\n",
    "          bias_axes=bias_axes if self._use_bias else None,\n",
    "          name=\"key\",\n",
    "          **common_kwargs)\n",
    "#       einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "#           self._value_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "#       self._value_dense = einsum_dense.EinsumDense(\n",
    "#           einsum_equation,\n",
    "#           output_shape=_get_output_shape(output_rank - 1,\n",
    "#                                          [self._num_heads, self._value_dim]),\n",
    "#           bias_axes=bias_axes if self._use_bias else None,\n",
    "#           name=\"value\",\n",
    "#           **common_kwargs)\n",
    "\n",
    "      # Builds the attention computations for multi-head dot product attention.\n",
    "      # These computations could be wrapped into the keras attention layer once\n",
    "      # it support mult-head einsum computations.\n",
    "      self._build_attention(output_rank)\n",
    "      self._output_dense = self._make_output_dense(\n",
    "          free_dims, common_kwargs, \"attention_output\")\n",
    "\n",
    "  def _make_output_dense(self, free_dims, common_kwargs, name=None):\n",
    "    \"\"\"Builds the output projection matrix.\n",
    "\n",
    "    Args:\n",
    "      free_dims: Number of free dimensions for einsum equation building.\n",
    "      common_kwargs: Common keyword arguments for einsum layer.\n",
    "      name: Name for the projection layer.\n",
    "\n",
    "    Returns:\n",
    "      Projection layer.\n",
    "    \"\"\"\n",
    "    if self._output_shape:\n",
    "      if not isinstance(self._output_shape, collections.abc.Sized):\n",
    "        output_shape = [self._output_shape]\n",
    "      else:\n",
    "        output_shape = self._output_shape\n",
    "    else:\n",
    "      output_shape = [self._query_shape[-1]]\n",
    "    einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "        free_dims, bound_dims=2, output_dims=len(output_shape))\n",
    "    return einsum_dense.EinsumDense(\n",
    "        einsum_equation,\n",
    "        output_shape=_get_output_shape(output_rank - 1, output_shape),\n",
    "        bias_axes=bias_axes if self._use_bias else None,\n",
    "        name=name,\n",
    "        **common_kwargs)\n",
    "\n",
    "  def _build_attention(self, rank):\n",
    "    \"\"\"Builds multi-head dot-product attention computations.\n",
    "\n",
    "    This function builds attributes necessary for `_compute_attention` to\n",
    "    costomize attention computation to replace the default dot-product\n",
    "    attention.\n",
    "\n",
    "    Args:\n",
    "      rank: the rank of query, key, value tensors.\n",
    "    \"\"\"\n",
    "    if self._attention_axes is None:\n",
    "      self._attention_axes = tuple(range(1, rank - 2))\n",
    "    else:\n",
    "      self._attention_axes = tuple(self._attention_axes)\n",
    "    self._dot_product_equation, self._combine_equation, attn_scores_rank = (\n",
    "        _build_attention_equation(rank, attn_axes=self._attention_axes))\n",
    "    norm_axes = tuple(\n",
    "        range(attn_scores_rank - len(self._attention_axes), attn_scores_rank))\n",
    "    self._softmax = advanced_activations.Softmax(axis=norm_axes)\n",
    "    self._dropout_layer = core.Dropout(rate=self._dropout)\n",
    "\n",
    "  def _masked_softmax(self, attention_scores, attention_mask=None):\n",
    "    # Normalize the attention scores to probabilities.\n",
    "    # `attention_scores` = [B, N, T, S]\n",
    "    if attention_mask is not None:\n",
    "      # The expand dim happens starting from the `num_heads` dimension,\n",
    "      # (<batch_dims>, num_heads, <query_attention_dims, key_attention_dims>)\n",
    "      mask_expansion_axis = -len(self._attention_axes) * 2 - 1\n",
    "      for _ in range(len(attention_scores.shape) - len(attention_mask.shape)):\n",
    "        attention_mask = tf.expand_dims(\n",
    "            attention_mask, axis=mask_expansion_axis)\n",
    "    return self._softmax(attention_scores, attention_mask)\n",
    "\n",
    "  def _compute_attention(self,\n",
    "                         query,\n",
    "                         key,\n",
    "                         attention_mask=None,\n",
    "                         training=None):\n",
    "    \"\"\"Applies Dot-product attention with query, key, value tensors.\n",
    "\n",
    "    This function defines the computation inside `call` with projected\n",
    "    multi-head Q, K, V inputs. Users can override this function for customized\n",
    "    attention implementation.\n",
    "\n",
    "    Args:\n",
    "      query: Projected query `Tensor` of shape `(B, T, N, key_dim)`.\n",
    "      key: Projected key `Tensor` of shape `(B, T, N, key_dim)`.\n",
    "      value: Projected value `Tensor` of shape `(B, T, N, value_dim)`.\n",
    "      attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
    "        attention to certain positions.\n",
    "      training: Python boolean indicating whether the layer should behave in\n",
    "        training mode (adding dropout) or in inference mode (doing nothing).\n",
    "\n",
    "    Returns:\n",
    "      attention_output: Multi-headed outputs of attention computation.\n",
    "      attention_scores: Multi-headed attention weights.\n",
    "    \"\"\"\n",
    "    # Note: Applying scalar multiply at the smaller end of einsum improves\n",
    "    # XLA performance, but may introduce slight numeric differences in\n",
    "    # the Transformer attention head.\n",
    "    query = tf.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "    # Take the dot product between \"query\" and \"key\" to get the raw\n",
    "    # attention scores.\n",
    "    attention_scores = tf.einsum(self._dot_product_equation, key,\n",
    "                                               query)\n",
    "\n",
    "    attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "    # This is actually dropping out entire tokens to attend to, which might\n",
    "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "    attention_scores_dropout = self._dropout_layer(\n",
    "        attention_scores, training=training)\n",
    "\n",
    "    # `context_layer` = [B, T, N, H]\n",
    "    attention_output = tf.einsum(self._combine_equation,\n",
    "                                               attention_scores_dropout, key)\n",
    "    return attention_output, attention_scores\n",
    "\n",
    "  def call(self,\n",
    "           query,\n",
    "           value,\n",
    "           key=None,\n",
    "           attention_mask=None,\n",
    "           return_attention_scores=False,\n",
    "           training=None):\n",
    "    if not self._built_from_signature:\n",
    "      self._build_from_signature(query=query, value=value, key=key)\n",
    "    if key is None:\n",
    "      key = value\n",
    "\n",
    "    #   N = `num_attention_heads`\n",
    "    #   H = `size_per_head`\n",
    "    # `query` = [B, T, N ,H]\n",
    "    query = self._query_dense(query)\n",
    "\n",
    "    # `key` = [B, S, N, H]\n",
    "    key = self._key_dense(key)\n",
    "\n",
    "    # `value` = [B, S, N, H]\n",
    "#     value = self._value_dense(value)\n",
    "\n",
    "    attention_output, attention_scores = self._compute_attention(\n",
    "        query, key, attention_mask, training)\n",
    "    attention_output = self._output_dense(attention_output)\n",
    "\n",
    "    if return_attention_scores:\n",
    "      return attention_output, attention_scores\n",
    "    return attention_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AhsnrfN7lF24"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSmzFg42lF24"
   },
   "source": [
    "## Compile, train, and evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " data_augmentation (Sequential)  (None, 72, 72, 3)   7           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " patches_1 (Patches)            (None, None, 108)    0           ['data_augmentation[0][0]']      \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 144, 64)      16192       ['patches_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 144, 64)     128         ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 144, 64)     49728       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 144, 64)      0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 144, 64)     128         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 144, 128)     8320        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 144, 128)     0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 144, 64)      8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 144, 64)      0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 144, 64)      0           ['dropout_1[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 144, 64)     128         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 144, 64)     49728       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 144, 64)      0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 144, 64)     128         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 144, 128)     8320        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 144, 128)     0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 144, 64)      8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 144, 64)      0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 144, 64)      0           ['dropout_3[0][0]',              \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 144, 64)     128         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 144, 64)     49728       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 144, 64)      0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 144, 64)     128         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 144, 128)     8320        ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 144, 128)     0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 144, 64)      8256        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 144, 64)      0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 144, 64)      0           ['dropout_5[0][0]',              \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 144, 64)     128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " multi_head_attention_3 (MultiH  (None, 144, 64)     49728       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 144, 64)      0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 144, 64)     128         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 144, 128)     8320        ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 144, 128)     0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 144, 64)      8256        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 144, 64)      0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 144, 64)      0           ['dropout_7[0][0]',              \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 144, 64)     128         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 144, 64)     49728       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 144, 64)      0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 144, 64)     128         ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 144, 128)     8320        ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 144, 128)     0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 144, 64)      8256        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 144, 64)      0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 144, 64)      0           ['dropout_9[0][0]',              \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 144, 64)     128         ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 144, 64)     49728       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 144, 64)      0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 144, 64)     128         ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 144, 128)     8320        ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 144, 128)     0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 144, 64)      8256        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 144, 64)      0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 144, 64)      0           ['dropout_11[0][0]',             \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 144, 64)     128         ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 144, 64)     49728       ['layer_normalization_12[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 144, 64)      0           ['multi_head_attention_6[0][0]', \n",
      "                                                                  'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 144, 64)     128         ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 144, 128)     8320        ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 144, 128)     0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 144, 64)      8256        ['dropout_12[0][0]']             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 144, 64)      0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 144, 64)      0           ['dropout_13[0][0]',             \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 144, 64)     128         ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 144, 64)     49728       ['layer_normalization_14[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 144, 64)      0           ['multi_head_attention_7[0][0]', \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 144, 64)     128         ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 144, 128)     8320        ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 144, 128)     0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 144, 64)      8256        ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 144, 64)      0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 144, 64)      0           ['dropout_15[0][0]',             \n",
      "                                                                  'add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 144, 64)     128         ['add_15[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 9216)         0           ['layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 9216)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 2048)         18876416    ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 2048)         0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 1024)         2098176     ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 1024)         0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 100)          102500      ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,625,899\n",
      "Trainable params: 21,625,892\n",
      "Non-trainable params: 7\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_vit_classifier().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21625899"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_vit_classifier().count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ujQwZ1-lF25",
    "outputId": "6443f9a2-76a2-4419-87f3-76ce589ac736",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/82\n",
      "352/352 [==============================] - 48s 98ms/step - loss: 4.4524 - accuracy: 0.0454 - top-5-accuracy: 0.1636 - val_loss: 3.8911 - val_accuracy: 0.1070 - val_top-5-accuracy: 0.3100\n",
      "Epoch 2/82\n",
      "352/352 [==============================] - 33s 95ms/step - loss: 3.9438 - accuracy: 0.0916 - top-5-accuracy: 0.2899 - val_loss: 3.5830 - val_accuracy: 0.1534 - val_top-5-accuracy: 0.4032\n",
      "Epoch 3/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 3.7230 - accuracy: 0.1264 - top-5-accuracy: 0.3581 - val_loss: 3.3897 - val_accuracy: 0.1818 - val_top-5-accuracy: 0.4562\n",
      "Epoch 4/82\n",
      "352/352 [==============================] - 34s 96ms/step - loss: 3.5565 - accuracy: 0.1534 - top-5-accuracy: 0.4074 - val_loss: 3.2464 - val_accuracy: 0.2100 - val_top-5-accuracy: 0.5014\n",
      "Epoch 5/82\n",
      "352/352 [==============================] - 33s 95ms/step - loss: 3.3846 - accuracy: 0.1812 - top-5-accuracy: 0.4561 - val_loss: 3.0445 - val_accuracy: 0.2524 - val_top-5-accuracy: 0.5464\n",
      "Epoch 6/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 3.2240 - accuracy: 0.2119 - top-5-accuracy: 0.5008 - val_loss: 2.9422 - val_accuracy: 0.2686 - val_top-5-accuracy: 0.5676\n",
      "Epoch 7/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 3.0851 - accuracy: 0.2378 - top-5-accuracy: 0.5351 - val_loss: 2.7655 - val_accuracy: 0.2984 - val_top-5-accuracy: 0.6122\n",
      "Epoch 8/82\n",
      "352/352 [==============================] - 33s 95ms/step - loss: 2.9586 - accuracy: 0.2647 - top-5-accuracy: 0.5644 - val_loss: 2.6675 - val_accuracy: 0.3246 - val_top-5-accuracy: 0.6346\n",
      "Epoch 9/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 2.8545 - accuracy: 0.2824 - top-5-accuracy: 0.5920 - val_loss: 2.6359 - val_accuracy: 0.3324 - val_top-5-accuracy: 0.6366\n",
      "Epoch 10/82\n",
      "352/352 [==============================] - 33s 93ms/step - loss: 2.7431 - accuracy: 0.3081 - top-5-accuracy: 0.6148 - val_loss: 2.5233 - val_accuracy: 0.3534 - val_top-5-accuracy: 0.6598\n",
      "Epoch 11/82\n",
      "352/352 [==============================] - 34s 97ms/step - loss: 2.6586 - accuracy: 0.3247 - top-5-accuracy: 0.6399 - val_loss: 2.4565 - val_accuracy: 0.3672 - val_top-5-accuracy: 0.6778\n",
      "Epoch 12/82\n",
      "352/352 [==============================] - 33s 95ms/step - loss: 2.5743 - accuracy: 0.3394 - top-5-accuracy: 0.6572 - val_loss: 2.3796 - val_accuracy: 0.3892 - val_top-5-accuracy: 0.6916\n",
      "Epoch 13/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 2.5101 - accuracy: 0.3572 - top-5-accuracy: 0.6706 - val_loss: 2.3405 - val_accuracy: 0.3984 - val_top-5-accuracy: 0.7004\n",
      "Epoch 14/82\n",
      "352/352 [==============================] - 33s 95ms/step - loss: 2.4383 - accuracy: 0.3681 - top-5-accuracy: 0.6829 - val_loss: 2.2573 - val_accuracy: 0.4132 - val_top-5-accuracy: 0.7176\n",
      "Epoch 15/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 2.3827 - accuracy: 0.3806 - top-5-accuracy: 0.6960 - val_loss: 2.3011 - val_accuracy: 0.4046 - val_top-5-accuracy: 0.7050\n",
      "Epoch 16/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 2.3293 - accuracy: 0.3934 - top-5-accuracy: 0.7092 - val_loss: 2.2257 - val_accuracy: 0.4314 - val_top-5-accuracy: 0.7208\n",
      "Epoch 17/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 2.2707 - accuracy: 0.4048 - top-5-accuracy: 0.7199 - val_loss: 2.1942 - val_accuracy: 0.4348 - val_top-5-accuracy: 0.7230\n",
      "Epoch 18/82\n",
      "352/352 [==============================] - 31s 89ms/step - loss: 2.2334 - accuracy: 0.4121 - top-5-accuracy: 0.7273 - val_loss: 2.1752 - val_accuracy: 0.4288 - val_top-5-accuracy: 0.7294\n",
      "Epoch 19/82\n",
      "352/352 [==============================] - 32s 90ms/step - loss: 2.1800 - accuracy: 0.4258 - top-5-accuracy: 0.7384 - val_loss: 2.1657 - val_accuracy: 0.4348 - val_top-5-accuracy: 0.7334\n",
      "Epoch 20/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 2.1460 - accuracy: 0.4339 - top-5-accuracy: 0.7463 - val_loss: 2.0774 - val_accuracy: 0.4566 - val_top-5-accuracy: 0.7518\n",
      "Epoch 21/82\n",
      "352/352 [==============================] - 32s 90ms/step - loss: 2.0993 - accuracy: 0.4425 - top-5-accuracy: 0.7559 - val_loss: 2.0866 - val_accuracy: 0.4558 - val_top-5-accuracy: 0.7518\n",
      "Epoch 22/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 2.0502 - accuracy: 0.4518 - top-5-accuracy: 0.7649 - val_loss: 2.0735 - val_accuracy: 0.4598 - val_top-5-accuracy: 0.7492\n",
      "Epoch 23/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 2.0274 - accuracy: 0.4611 - top-5-accuracy: 0.7694 - val_loss: 2.0212 - val_accuracy: 0.4666 - val_top-5-accuracy: 0.7612\n",
      "Epoch 24/82\n",
      "352/352 [==============================] - 32s 90ms/step - loss: 1.9915 - accuracy: 0.4634 - top-5-accuracy: 0.7751 - val_loss: 2.0421 - val_accuracy: 0.4590 - val_top-5-accuracy: 0.7564\n",
      "Epoch 25/82\n",
      "352/352 [==============================] - 33s 95ms/step - loss: 1.9575 - accuracy: 0.4752 - top-5-accuracy: 0.7831 - val_loss: 2.0134 - val_accuracy: 0.4682 - val_top-5-accuracy: 0.7628\n",
      "Epoch 26/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.9173 - accuracy: 0.4807 - top-5-accuracy: 0.7918 - val_loss: 2.0152 - val_accuracy: 0.4656 - val_top-5-accuracy: 0.7646\n",
      "Epoch 27/82\n",
      "352/352 [==============================] - 33s 95ms/step - loss: 1.8848 - accuracy: 0.4907 - top-5-accuracy: 0.7948 - val_loss: 1.9963 - val_accuracy: 0.4736 - val_top-5-accuracy: 0.7680\n",
      "Epoch 28/82\n",
      "352/352 [==============================] - 33s 93ms/step - loss: 1.8686 - accuracy: 0.4932 - top-5-accuracy: 0.7982 - val_loss: 1.9793 - val_accuracy: 0.4754 - val_top-5-accuracy: 0.7696\n",
      "Epoch 29/82\n",
      "352/352 [==============================] - 33s 93ms/step - loss: 1.8299 - accuracy: 0.5019 - top-5-accuracy: 0.8067 - val_loss: 1.9386 - val_accuracy: 0.4810 - val_top-5-accuracy: 0.7824\n",
      "Epoch 30/82\n",
      "352/352 [==============================] - 33s 93ms/step - loss: 1.8008 - accuracy: 0.5097 - top-5-accuracy: 0.8112 - val_loss: 1.9149 - val_accuracy: 0.4944 - val_top-5-accuracy: 0.7826\n",
      "Epoch 31/82\n",
      "352/352 [==============================] - 32s 90ms/step - loss: 1.7558 - accuracy: 0.5190 - top-5-accuracy: 0.8209 - val_loss: 1.9658 - val_accuracy: 0.4898 - val_top-5-accuracy: 0.7766\n",
      "Epoch 32/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.7440 - accuracy: 0.5228 - top-5-accuracy: 0.8224 - val_loss: 1.9299 - val_accuracy: 0.4940 - val_top-5-accuracy: 0.7830\n",
      "Epoch 33/82\n",
      "352/352 [==============================] - 32s 90ms/step - loss: 1.7084 - accuracy: 0.5277 - top-5-accuracy: 0.8276 - val_loss: 1.9178 - val_accuracy: 0.4936 - val_top-5-accuracy: 0.7866\n",
      "Epoch 34/82\n",
      "352/352 [==============================] - 32s 90ms/step - loss: 1.6899 - accuracy: 0.5341 - top-5-accuracy: 0.8304 - val_loss: 1.9256 - val_accuracy: 0.4928 - val_top-5-accuracy: 0.7810\n",
      "Epoch 35/82\n",
      "352/352 [==============================] - 33s 92ms/step - loss: 1.6593 - accuracy: 0.5418 - top-5-accuracy: 0.8395 - val_loss: 1.8609 - val_accuracy: 0.5100 - val_top-5-accuracy: 0.7924\n",
      "Epoch 36/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.6382 - accuracy: 0.5453 - top-5-accuracy: 0.8422 - val_loss: 1.8902 - val_accuracy: 0.5084 - val_top-5-accuracy: 0.7920\n",
      "Epoch 37/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.6242 - accuracy: 0.5483 - top-5-accuracy: 0.8442 - val_loss: 1.8757 - val_accuracy: 0.5070 - val_top-5-accuracy: 0.7926\n",
      "Epoch 38/82\n",
      "352/352 [==============================] - 33s 93ms/step - loss: 1.5900 - accuracy: 0.5570 - top-5-accuracy: 0.8491 - val_loss: 1.8424 - val_accuracy: 0.5168 - val_top-5-accuracy: 0.7984\n",
      "Epoch 39/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.5707 - accuracy: 0.5617 - top-5-accuracy: 0.8524 - val_loss: 1.9096 - val_accuracy: 0.5062 - val_top-5-accuracy: 0.7872\n",
      "Epoch 40/82\n",
      "352/352 [==============================] - 32s 90ms/step - loss: 1.5558 - accuracy: 0.5671 - top-5-accuracy: 0.8546 - val_loss: 1.8428 - val_accuracy: 0.5168 - val_top-5-accuracy: 0.8024\n",
      "Epoch 41/82\n",
      "352/352 [==============================] - 32s 90ms/step - loss: 1.5244 - accuracy: 0.5740 - top-5-accuracy: 0.8596 - val_loss: 1.8447 - val_accuracy: 0.5136 - val_top-5-accuracy: 0.8016\n",
      "Epoch 42/82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 32s 91ms/step - loss: 1.5102 - accuracy: 0.5770 - top-5-accuracy: 0.8634 - val_loss: 1.8419 - val_accuracy: 0.5110 - val_top-5-accuracy: 0.7958\n",
      "Epoch 43/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.5008 - accuracy: 0.5783 - top-5-accuracy: 0.8658 - val_loss: 1.8348 - val_accuracy: 0.5150 - val_top-5-accuracy: 0.8002\n",
      "Epoch 44/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 1.4755 - accuracy: 0.5812 - top-5-accuracy: 0.8703 - val_loss: 1.8441 - val_accuracy: 0.5226 - val_top-5-accuracy: 0.7924\n",
      "Epoch 45/82\n",
      "352/352 [==============================] - 33s 93ms/step - loss: 1.4695 - accuracy: 0.5856 - top-5-accuracy: 0.8694 - val_loss: 1.8300 - val_accuracy: 0.5258 - val_top-5-accuracy: 0.8024\n",
      "Epoch 46/82\n",
      "352/352 [==============================] - 32s 90ms/step - loss: 1.4587 - accuracy: 0.5869 - top-5-accuracy: 0.8738 - val_loss: 1.8596 - val_accuracy: 0.5184 - val_top-5-accuracy: 0.8074\n",
      "Epoch 47/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.4317 - accuracy: 0.5943 - top-5-accuracy: 0.8740 - val_loss: 1.8869 - val_accuracy: 0.5072 - val_top-5-accuracy: 0.8002\n",
      "Epoch 48/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.4217 - accuracy: 0.5987 - top-5-accuracy: 0.8773 - val_loss: 1.8823 - val_accuracy: 0.5182 - val_top-5-accuracy: 0.7998\n",
      "Epoch 49/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.4268 - accuracy: 0.5963 - top-5-accuracy: 0.8753 - val_loss: 1.8607 - val_accuracy: 0.5194 - val_top-5-accuracy: 0.8002\n",
      "Epoch 50/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.3846 - accuracy: 0.6068 - top-5-accuracy: 0.8814 - val_loss: 1.8447 - val_accuracy: 0.5242 - val_top-5-accuracy: 0.8044\n",
      "Epoch 51/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.3875 - accuracy: 0.6078 - top-5-accuracy: 0.8823 - val_loss: 1.8471 - val_accuracy: 0.5222 - val_top-5-accuracy: 0.8000\n",
      "Epoch 52/82\n",
      "352/352 [==============================] - 33s 93ms/step - loss: 1.3670 - accuracy: 0.6123 - top-5-accuracy: 0.8861 - val_loss: 1.8082 - val_accuracy: 0.5282 - val_top-5-accuracy: 0.8078\n",
      "Epoch 53/82\n",
      "352/352 [==============================] - 32s 92ms/step - loss: 1.3517 - accuracy: 0.6115 - top-5-accuracy: 0.8897 - val_loss: 1.8217 - val_accuracy: 0.5220 - val_top-5-accuracy: 0.8002\n",
      "Epoch 54/82\n",
      "352/352 [==============================] - 32s 90ms/step - loss: 1.3529 - accuracy: 0.6128 - top-5-accuracy: 0.8887 - val_loss: 1.8678 - val_accuracy: 0.5204 - val_top-5-accuracy: 0.7950\n",
      "Epoch 55/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.3308 - accuracy: 0.6184 - top-5-accuracy: 0.8909 - val_loss: 1.8261 - val_accuracy: 0.5266 - val_top-5-accuracy: 0.8112\n",
      "Epoch 56/82\n",
      "352/352 [==============================] - 32s 92ms/step - loss: 1.3299 - accuracy: 0.6206 - top-5-accuracy: 0.8915 - val_loss: 1.8209 - val_accuracy: 0.5222 - val_top-5-accuracy: 0.8042\n",
      "Epoch 57/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.3202 - accuracy: 0.6202 - top-5-accuracy: 0.8952 - val_loss: 1.8665 - val_accuracy: 0.5280 - val_top-5-accuracy: 0.8014\n",
      "Epoch 58/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.3196 - accuracy: 0.6234 - top-5-accuracy: 0.8942 - val_loss: 1.8273 - val_accuracy: 0.5278 - val_top-5-accuracy: 0.8066\n",
      "Epoch 59/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.2995 - accuracy: 0.6292 - top-5-accuracy: 0.8959 - val_loss: 1.8456 - val_accuracy: 0.5200 - val_top-5-accuracy: 0.8008\n",
      "Epoch 60/82\n",
      "352/352 [==============================] - 32s 92ms/step - loss: 1.3024 - accuracy: 0.6261 - top-5-accuracy: 0.8965 - val_loss: 1.8837 - val_accuracy: 0.5210 - val_top-5-accuracy: 0.8016\n",
      "Epoch 61/82\n",
      "352/352 [==============================] - 33s 93ms/step - loss: 1.2823 - accuracy: 0.6301 - top-5-accuracy: 0.8990 - val_loss: 1.8058 - val_accuracy: 0.5312 - val_top-5-accuracy: 0.8106\n",
      "Epoch 62/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.2716 - accuracy: 0.6345 - top-5-accuracy: 0.8991 - val_loss: 1.8404 - val_accuracy: 0.5260 - val_top-5-accuracy: 0.8102\n",
      "Epoch 63/82\n",
      "352/352 [==============================] - 32s 92ms/step - loss: 1.2661 - accuracy: 0.6357 - top-5-accuracy: 0.9009 - val_loss: 1.8659 - val_accuracy: 0.5298 - val_top-5-accuracy: 0.8034\n",
      "Epoch 64/82\n",
      "352/352 [==============================] - 33s 94ms/step - loss: 1.2551 - accuracy: 0.6400 - top-5-accuracy: 0.9035 - val_loss: 1.8173 - val_accuracy: 0.5336 - val_top-5-accuracy: 0.8152\n",
      "Epoch 65/82\n",
      "352/352 [==============================] - 32s 91ms/step - loss: 1.2559 - accuracy: 0.6373 - top-5-accuracy: 0.9016 - val_loss: 1.8540 - val_accuracy: 0.5308 - val_top-5-accuracy: 0.8070\n",
      "Epoch 66/82\n",
      "352/352 [==============================] - 32s 92ms/step - loss: 1.2667 - accuracy: 0.6386 - top-5-accuracy: 0.9028 - val_loss: 1.8351 - val_accuracy: 0.5258 - val_top-5-accuracy: 0.8082\n",
      "Epoch 67/82\n",
      "352/352 [==============================] - 32s 90ms/step - loss: 1.2338 - accuracy: 0.6469 - top-5-accuracy: 0.9052 - val_loss: 1.8389 - val_accuracy: 0.5278 - val_top-5-accuracy: 0.8054\n",
      "Epoch 68/82\n",
      "352/352 [==============================] - 31s 87ms/step - loss: 1.2472 - accuracy: 0.6419 - top-5-accuracy: 0.9042 - val_loss: 1.8530 - val_accuracy: 0.5302 - val_top-5-accuracy: 0.8102\n",
      "Epoch 69/82\n",
      "352/352 [==============================] - 29s 83ms/step - loss: 1.2199 - accuracy: 0.6503 - top-5-accuracy: 0.9070 - val_loss: 1.8392 - val_accuracy: 0.5268 - val_top-5-accuracy: 0.8026\n",
      "Epoch 70/82\n",
      "352/352 [==============================] - 28s 80ms/step - loss: 1.2115 - accuracy: 0.6509 - top-5-accuracy: 0.9073 - val_loss: 1.8547 - val_accuracy: 0.5324 - val_top-5-accuracy: 0.8008\n",
      "Epoch 71/82\n",
      "352/352 [==============================] - 29s 82ms/step - loss: 1.2172 - accuracy: 0.6485 - top-5-accuracy: 0.9071 - val_loss: 1.8064 - val_accuracy: 0.5348 - val_top-5-accuracy: 0.8148\n",
      "Epoch 72/82\n",
      "352/352 [==============================] - 28s 80ms/step - loss: 1.2012 - accuracy: 0.6552 - top-5-accuracy: 0.9103 - val_loss: 1.8415 - val_accuracy: 0.5348 - val_top-5-accuracy: 0.8114\n",
      "Epoch 73/82\n",
      "352/352 [==============================] - 28s 79ms/step - loss: 1.2002 - accuracy: 0.6530 - top-5-accuracy: 0.9091 - val_loss: 1.8718 - val_accuracy: 0.5244 - val_top-5-accuracy: 0.8024\n",
      "Epoch 74/82\n",
      "352/352 [==============================] - 28s 79ms/step - loss: 1.1935 - accuracy: 0.6576 - top-5-accuracy: 0.9107 - val_loss: 1.8026 - val_accuracy: 0.5328 - val_top-5-accuracy: 0.8048\n",
      "Epoch 75/82\n",
      "352/352 [==============================] - 28s 79ms/step - loss: 1.1880 - accuracy: 0.6568 - top-5-accuracy: 0.9120 - val_loss: 1.8608 - val_accuracy: 0.5288 - val_top-5-accuracy: 0.8062\n",
      "Epoch 76/82\n",
      "352/352 [==============================] - 28s 78ms/step - loss: 1.2009 - accuracy: 0.6529 - top-5-accuracy: 0.9101 - val_loss: 1.8252 - val_accuracy: 0.5276 - val_top-5-accuracy: 0.8046\n",
      "Epoch 77/82\n",
      "352/352 [==============================] - 28s 78ms/step - loss: 1.1838 - accuracy: 0.6599 - top-5-accuracy: 0.9145 - val_loss: 1.8512 - val_accuracy: 0.5302 - val_top-5-accuracy: 0.8102\n",
      "Epoch 78/82\n",
      "352/352 [==============================] - 27s 78ms/step - loss: 1.1740 - accuracy: 0.6610 - top-5-accuracy: 0.9155 - val_loss: 1.8325 - val_accuracy: 0.5310 - val_top-5-accuracy: 0.8142\n",
      "Epoch 79/82\n",
      "352/352 [==============================] - 27s 78ms/step - loss: 1.1730 - accuracy: 0.6601 - top-5-accuracy: 0.9131 - val_loss: 1.8528 - val_accuracy: 0.5388 - val_top-5-accuracy: 0.8108\n",
      "Epoch 80/82\n",
      "352/352 [==============================] - 27s 76ms/step - loss: 1.1571 - accuracy: 0.6653 - top-5-accuracy: 0.9166 - val_loss: 1.8645 - val_accuracy: 0.5178 - val_top-5-accuracy: 0.8032\n",
      "Epoch 81/82\n",
      "352/352 [==============================] - 27s 76ms/step - loss: 1.1643 - accuracy: 0.6656 - top-5-accuracy: 0.9146 - val_loss: 1.8492 - val_accuracy: 0.5326 - val_top-5-accuracy: 0.8076\n",
      "Epoch 82/82\n",
      "352/352 [==============================] - 27s 76ms/step - loss: 1.1539 - accuracy: 0.6639 - top-5-accuracy: 0.9169 - val_loss: 1.8329 - val_accuracy: 0.5320 - val_top-5-accuracy: 0.8060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 14ms/step - loss: 1.8108 - accuracy: 0.5383 - top-5-accuracy: 0.8096\n",
      "Test accuracy: 53.83%\n",
      "Test top 5 accuracy: 80.96%\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy: 53.83%\n",
    "Test top 5 accuracy: 80.96%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42.5m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2=history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'top-5-accuracy', 'val_loss', 'val_accuracy', 'val_top-5-accuracy'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [4.452352046966553,\n",
       "  3.943793296813965,\n",
       "  3.7229535579681396,\n",
       "  3.55649995803833,\n",
       "  3.384573221206665,\n",
       "  3.2239837646484375,\n",
       "  3.085104465484619,\n",
       "  2.9586148262023926,\n",
       "  2.854482889175415,\n",
       "  2.743145227432251,\n",
       "  2.6586384773254395,\n",
       "  2.5742626190185547,\n",
       "  2.5100910663604736,\n",
       "  2.4382834434509277,\n",
       "  2.382707357406616,\n",
       "  2.3293063640594482,\n",
       "  2.2707290649414062,\n",
       "  2.23337984085083,\n",
       "  2.1799886226654053,\n",
       "  2.145975112915039,\n",
       "  2.099308729171753,\n",
       "  2.0501949787139893,\n",
       "  2.027372121810913,\n",
       "  1.9915083646774292,\n",
       "  1.9575165510177612,\n",
       "  1.9173182249069214,\n",
       "  1.8847951889038086,\n",
       "  1.8685612678527832,\n",
       "  1.8299354314804077,\n",
       "  1.8007736206054688,\n",
       "  1.7558296918869019,\n",
       "  1.7439895868301392,\n",
       "  1.7084288597106934,\n",
       "  1.6898571252822876,\n",
       "  1.6592668294906616,\n",
       "  1.6381616592407227,\n",
       "  1.624237298965454,\n",
       "  1.5899815559387207,\n",
       "  1.570650339126587,\n",
       "  1.5557572841644287,\n",
       "  1.5244383811950684,\n",
       "  1.510156273841858,\n",
       "  1.500809907913208,\n",
       "  1.475530743598938,\n",
       "  1.4694504737854004,\n",
       "  1.458735466003418,\n",
       "  1.4317021369934082,\n",
       "  1.4216874837875366,\n",
       "  1.4268414974212646,\n",
       "  1.3846062421798706,\n",
       "  1.3874776363372803,\n",
       "  1.3669990301132202,\n",
       "  1.3517377376556396,\n",
       "  1.352923035621643,\n",
       "  1.3307826519012451,\n",
       "  1.329870581626892,\n",
       "  1.3202189207077026,\n",
       "  1.319583535194397,\n",
       "  1.2994791269302368,\n",
       "  1.3024131059646606,\n",
       "  1.2823474407196045,\n",
       "  1.2716161012649536,\n",
       "  1.2661242485046387,\n",
       "  1.2551369667053223,\n",
       "  1.255873680114746,\n",
       "  1.2666622400283813,\n",
       "  1.2337627410888672,\n",
       "  1.2471601963043213,\n",
       "  1.2199461460113525,\n",
       "  1.211473822593689,\n",
       "  1.2172023057937622,\n",
       "  1.201192855834961,\n",
       "  1.2002497911453247,\n",
       "  1.193529725074768,\n",
       "  1.1879733800888062,\n",
       "  1.2009373903274536,\n",
       "  1.1838276386260986,\n",
       "  1.1740291118621826,\n",
       "  1.1730446815490723,\n",
       "  1.1570637226104736,\n",
       "  1.1642963886260986,\n",
       "  1.1539244651794434],\n",
       " 'accuracy': [0.04544444382190704,\n",
       "  0.09157777577638626,\n",
       "  0.12635555863380432,\n",
       "  0.15337777137756348,\n",
       "  0.18115556240081787,\n",
       "  0.21193332970142365,\n",
       "  0.23784443736076355,\n",
       "  0.2646888792514801,\n",
       "  0.2824000120162964,\n",
       "  0.3081111013889313,\n",
       "  0.3247111141681671,\n",
       "  0.3393999934196472,\n",
       "  0.3571777641773224,\n",
       "  0.3681333363056183,\n",
       "  0.38057777285575867,\n",
       "  0.39340001344680786,\n",
       "  0.404755562543869,\n",
       "  0.4121333360671997,\n",
       "  0.4258444309234619,\n",
       "  0.43388888239860535,\n",
       "  0.44253334403038025,\n",
       "  0.4518444538116455,\n",
       "  0.46113333106040955,\n",
       "  0.46335554122924805,\n",
       "  0.4751555621623993,\n",
       "  0.48071110248565674,\n",
       "  0.4907333254814148,\n",
       "  0.4931555688381195,\n",
       "  0.5019111037254333,\n",
       "  0.5097333192825317,\n",
       "  0.5190222263336182,\n",
       "  0.5228000283241272,\n",
       "  0.5276666879653931,\n",
       "  0.5340889096260071,\n",
       "  0.5417777895927429,\n",
       "  0.5453333258628845,\n",
       "  0.5483333468437195,\n",
       "  0.557022213935852,\n",
       "  0.5617111325263977,\n",
       "  0.5671333074569702,\n",
       "  0.5740222334861755,\n",
       "  0.5769555568695068,\n",
       "  0.57833331823349,\n",
       "  0.5812444686889648,\n",
       "  0.5856444239616394,\n",
       "  0.5869333148002625,\n",
       "  0.5942666530609131,\n",
       "  0.5986889004707336,\n",
       "  0.596311092376709,\n",
       "  0.606844425201416,\n",
       "  0.6077777743339539,\n",
       "  0.6122888922691345,\n",
       "  0.611466646194458,\n",
       "  0.6127777695655823,\n",
       "  0.6184444427490234,\n",
       "  0.6206444501876831,\n",
       "  0.6202444434165955,\n",
       "  0.6234444379806519,\n",
       "  0.6291555762290955,\n",
       "  0.6260889172554016,\n",
       "  0.630133330821991,\n",
       "  0.6345333456993103,\n",
       "  0.6356889009475708,\n",
       "  0.6399555802345276,\n",
       "  0.6373111009597778,\n",
       "  0.6386222243309021,\n",
       "  0.646911084651947,\n",
       "  0.6419333219528198,\n",
       "  0.6503333449363708,\n",
       "  0.6508888602256775,\n",
       "  0.6484666466712952,\n",
       "  0.6552222371101379,\n",
       "  0.652999997138977,\n",
       "  0.657622218132019,\n",
       "  0.6567999720573425,\n",
       "  0.6528666615486145,\n",
       "  0.6598666906356812,\n",
       "  0.6610444188117981,\n",
       "  0.6601333618164062,\n",
       "  0.6652888655662537,\n",
       "  0.6656000018119812,\n",
       "  0.6638888716697693],\n",
       " 'top-5-accuracy': [0.1636444479227066,\n",
       "  0.28993332386016846,\n",
       "  0.358088880777359,\n",
       "  0.40737777948379517,\n",
       "  0.4560889005661011,\n",
       "  0.500844419002533,\n",
       "  0.5350666642189026,\n",
       "  0.5644444227218628,\n",
       "  0.5920000076293945,\n",
       "  0.6148444414138794,\n",
       "  0.6398666501045227,\n",
       "  0.6571999788284302,\n",
       "  0.6706444621086121,\n",
       "  0.6829110980033875,\n",
       "  0.6960444450378418,\n",
       "  0.7092221975326538,\n",
       "  0.7198666930198669,\n",
       "  0.7273333072662354,\n",
       "  0.7383999824523926,\n",
       "  0.7462666630744934,\n",
       "  0.7559333443641663,\n",
       "  0.7649333477020264,\n",
       "  0.7693777680397034,\n",
       "  0.7751111388206482,\n",
       "  0.783133327960968,\n",
       "  0.7918221950531006,\n",
       "  0.7947777509689331,\n",
       "  0.7981777787208557,\n",
       "  0.8067333102226257,\n",
       "  0.8112221956253052,\n",
       "  0.8208666443824768,\n",
       "  0.8224444389343262,\n",
       "  0.8276444673538208,\n",
       "  0.8303999900817871,\n",
       "  0.8395110964775085,\n",
       "  0.842199981212616,\n",
       "  0.8442000150680542,\n",
       "  0.8490889072418213,\n",
       "  0.8524444699287415,\n",
       "  0.8545555472373962,\n",
       "  0.8596444725990295,\n",
       "  0.8633555769920349,\n",
       "  0.865755558013916,\n",
       "  0.8702666759490967,\n",
       "  0.869422197341919,\n",
       "  0.8738222122192383,\n",
       "  0.8739555478096008,\n",
       "  0.8773333430290222,\n",
       "  0.8752889037132263,\n",
       "  0.8813555836677551,\n",
       "  0.8823333382606506,\n",
       "  0.8861111402511597,\n",
       "  0.8896666765213013,\n",
       "  0.8886666893959045,\n",
       "  0.8908888697624207,\n",
       "  0.8914666771888733,\n",
       "  0.8951555490493774,\n",
       "  0.8942221999168396,\n",
       "  0.8959333300590515,\n",
       "  0.8965333104133606,\n",
       "  0.8990222215652466,\n",
       "  0.899066686630249,\n",
       "  0.9008888602256775,\n",
       "  0.9035333395004272,\n",
       "  0.9015777707099915,\n",
       "  0.9027777910232544,\n",
       "  0.9052222371101379,\n",
       "  0.9041555523872375,\n",
       "  0.9070000052452087,\n",
       "  0.9073110818862915,\n",
       "  0.9070888757705688,\n",
       "  0.9103333353996277,\n",
       "  0.9090889096260071,\n",
       "  0.9106888771057129,\n",
       "  0.9120444655418396,\n",
       "  0.9101333618164062,\n",
       "  0.9144889116287231,\n",
       "  0.9155333042144775,\n",
       "  0.9131333231925964,\n",
       "  0.9166444540023804,\n",
       "  0.9145777821540833,\n",
       "  0.9169111251831055],\n",
       " 'val_loss': [3.8910789489746094,\n",
       "  3.582974910736084,\n",
       "  3.3896734714508057,\n",
       "  3.246363639831543,\n",
       "  3.044456958770752,\n",
       "  2.9422378540039062,\n",
       "  2.7654905319213867,\n",
       "  2.667459726333618,\n",
       "  2.6359426975250244,\n",
       "  2.523308515548706,\n",
       "  2.4564669132232666,\n",
       "  2.3795700073242188,\n",
       "  2.340547800064087,\n",
       "  2.257338523864746,\n",
       "  2.301112413406372,\n",
       "  2.2257282733917236,\n",
       "  2.1942315101623535,\n",
       "  2.175201654434204,\n",
       "  2.1657016277313232,\n",
       "  2.0773861408233643,\n",
       "  2.086613893508911,\n",
       "  2.0735299587249756,\n",
       "  2.0212466716766357,\n",
       "  2.042104721069336,\n",
       "  2.0133588314056396,\n",
       "  2.015228271484375,\n",
       "  1.996341347694397,\n",
       "  1.9793092012405396,\n",
       "  1.9385511875152588,\n",
       "  1.9148751497268677,\n",
       "  1.9657642841339111,\n",
       "  1.9298661947250366,\n",
       "  1.9178056716918945,\n",
       "  1.925605058670044,\n",
       "  1.8609352111816406,\n",
       "  1.8901584148406982,\n",
       "  1.8756736516952515,\n",
       "  1.8424237966537476,\n",
       "  1.9095866680145264,\n",
       "  1.8428043127059937,\n",
       "  1.8447214365005493,\n",
       "  1.841944694519043,\n",
       "  1.834808588027954,\n",
       "  1.844138503074646,\n",
       "  1.82999587059021,\n",
       "  1.859622597694397,\n",
       "  1.886874794960022,\n",
       "  1.882275938987732,\n",
       "  1.8606929779052734,\n",
       "  1.8447479009628296,\n",
       "  1.8471144437789917,\n",
       "  1.8082211017608643,\n",
       "  1.8216882944107056,\n",
       "  1.867752194404602,\n",
       "  1.8260620832443237,\n",
       "  1.8209478855133057,\n",
       "  1.866466760635376,\n",
       "  1.8272589445114136,\n",
       "  1.8456484079360962,\n",
       "  1.883672833442688,\n",
       "  1.8057974576950073,\n",
       "  1.8404062986373901,\n",
       "  1.8658685684204102,\n",
       "  1.8173103332519531,\n",
       "  1.853973627090454,\n",
       "  1.8350505828857422,\n",
       "  1.8389087915420532,\n",
       "  1.8530113697052002,\n",
       "  1.8392324447631836,\n",
       "  1.8546990156173706,\n",
       "  1.806360125541687,\n",
       "  1.8415378332138062,\n",
       "  1.8717588186264038,\n",
       "  1.8026297092437744,\n",
       "  1.8608123064041138,\n",
       "  1.8251953125,\n",
       "  1.851243495941162,\n",
       "  1.832543969154358,\n",
       "  1.8528434038162231,\n",
       "  1.8645150661468506,\n",
       "  1.8492045402526855,\n",
       "  1.8329076766967773],\n",
       " 'val_accuracy': [0.10700000077486038,\n",
       "  0.1534000039100647,\n",
       "  0.1817999929189682,\n",
       "  0.20999999344348907,\n",
       "  0.2524000108242035,\n",
       "  0.2685999870300293,\n",
       "  0.29840001463890076,\n",
       "  0.3246000111103058,\n",
       "  0.33239999413490295,\n",
       "  0.35339999198913574,\n",
       "  0.36719998717308044,\n",
       "  0.38920000195503235,\n",
       "  0.3984000086784363,\n",
       "  0.4131999909877777,\n",
       "  0.40459999442100525,\n",
       "  0.43140000104904175,\n",
       "  0.43479999899864197,\n",
       "  0.42879998683929443,\n",
       "  0.43479999899864197,\n",
       "  0.45660001039505005,\n",
       "  0.45579999685287476,\n",
       "  0.45980000495910645,\n",
       "  0.4666000008583069,\n",
       "  0.45899999141693115,\n",
       "  0.4681999981403351,\n",
       "  0.46560001373291016,\n",
       "  0.47360000014305115,\n",
       "  0.47540000081062317,\n",
       "  0.48100000619888306,\n",
       "  0.4943999946117401,\n",
       "  0.48980000615119934,\n",
       "  0.49399998784065247,\n",
       "  0.4936000108718872,\n",
       "  0.4927999973297119,\n",
       "  0.5099999904632568,\n",
       "  0.508400022983551,\n",
       "  0.5070000290870667,\n",
       "  0.5167999863624573,\n",
       "  0.5062000155448914,\n",
       "  0.5167999863624573,\n",
       "  0.5135999917984009,\n",
       "  0.5109999775886536,\n",
       "  0.5149999856948853,\n",
       "  0.522599995136261,\n",
       "  0.5257999897003174,\n",
       "  0.5184000134468079,\n",
       "  0.5072000026702881,\n",
       "  0.5181999802589417,\n",
       "  0.5194000005722046,\n",
       "  0.5242000222206116,\n",
       "  0.5221999883651733,\n",
       "  0.5281999707221985,\n",
       "  0.5220000147819519,\n",
       "  0.5203999876976013,\n",
       "  0.5266000032424927,\n",
       "  0.5221999883651733,\n",
       "  0.527999997138977,\n",
       "  0.5278000235557556,\n",
       "  0.5199999809265137,\n",
       "  0.5210000276565552,\n",
       "  0.5311999917030334,\n",
       "  0.5260000228881836,\n",
       "  0.5297999978065491,\n",
       "  0.5335999727249146,\n",
       "  0.5307999849319458,\n",
       "  0.5257999897003174,\n",
       "  0.5278000235557556,\n",
       "  0.5302000045776367,\n",
       "  0.5267999768257141,\n",
       "  0.5324000120162964,\n",
       "  0.5347999930381775,\n",
       "  0.5347999930381775,\n",
       "  0.524399995803833,\n",
       "  0.532800018787384,\n",
       "  0.5288000106811523,\n",
       "  0.5275999903678894,\n",
       "  0.5302000045776367,\n",
       "  0.531000018119812,\n",
       "  0.5388000011444092,\n",
       "  0.517799973487854,\n",
       "  0.5325999855995178,\n",
       "  0.5320000052452087],\n",
       " 'val_top-5-accuracy': [0.3100000023841858,\n",
       "  0.4032000005245209,\n",
       "  0.4562000036239624,\n",
       "  0.5013999938964844,\n",
       "  0.5464000105857849,\n",
       "  0.5676000118255615,\n",
       "  0.6122000217437744,\n",
       "  0.6345999836921692,\n",
       "  0.6366000175476074,\n",
       "  0.6597999930381775,\n",
       "  0.6777999997138977,\n",
       "  0.6916000247001648,\n",
       "  0.7003999948501587,\n",
       "  0.7175999879837036,\n",
       "  0.7049999833106995,\n",
       "  0.72079998254776,\n",
       "  0.7229999899864197,\n",
       "  0.7293999791145325,\n",
       "  0.7333999872207642,\n",
       "  0.751800000667572,\n",
       "  0.751800000667572,\n",
       "  0.7491999864578247,\n",
       "  0.7612000107765198,\n",
       "  0.7563999891281128,\n",
       "  0.7627999782562256,\n",
       "  0.7645999789237976,\n",
       "  0.7680000066757202,\n",
       "  0.769599974155426,\n",
       "  0.7824000120162964,\n",
       "  0.7825999855995178,\n",
       "  0.7766000032424927,\n",
       "  0.7829999923706055,\n",
       "  0.7865999937057495,\n",
       "  0.781000018119812,\n",
       "  0.7924000024795532,\n",
       "  0.7919999957084656,\n",
       "  0.7925999760627747,\n",
       "  0.7983999848365784,\n",
       "  0.7871999740600586,\n",
       "  0.8023999929428101,\n",
       "  0.8015999794006348,\n",
       "  0.795799970626831,\n",
       "  0.8001999855041504,\n",
       "  0.7924000024795532,\n",
       "  0.8023999929428101,\n",
       "  0.8073999881744385,\n",
       "  0.8001999855041504,\n",
       "  0.7997999787330627,\n",
       "  0.8001999855041504,\n",
       "  0.8044000267982483,\n",
       "  0.800000011920929,\n",
       "  0.8077999949455261,\n",
       "  0.8001999855041504,\n",
       "  0.7950000166893005,\n",
       "  0.8112000226974487,\n",
       "  0.8041999936103821,\n",
       "  0.8014000058174133,\n",
       "  0.8065999746322632,\n",
       "  0.8008000254631042,\n",
       "  0.8015999794006348,\n",
       "  0.8105999827384949,\n",
       "  0.8101999759674072,\n",
       "  0.8033999800682068,\n",
       "  0.8151999711990356,\n",
       "  0.8069999814033508,\n",
       "  0.8082000017166138,\n",
       "  0.805400013923645,\n",
       "  0.8101999759674072,\n",
       "  0.8026000261306763,\n",
       "  0.8008000254631042,\n",
       "  0.8148000240325928,\n",
       "  0.8113999962806702,\n",
       "  0.8023999929428101,\n",
       "  0.8047999739646912,\n",
       "  0.8062000274658203,\n",
       "  0.8046000003814697,\n",
       "  0.8101999759674072,\n",
       "  0.8141999840736389,\n",
       "  0.8108000159263611,\n",
       "  0.8032000064849854,\n",
       "  0.8076000213623047,\n",
       "  0.8059999942779541]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4XQi3l4lF25"
   },
   "source": [
    "After 100 epochs, the ViT model achieves around 55% accuracy and\n",
    "82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n",
    "as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n",
    "\n",
    "Note that the state of the art results reported in the\n",
    "[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n",
    "the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n",
    "without pre-training, you can try to train the model for more epochs, use a larger number of\n",
    "Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. \n",
    "Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, \n",
    "but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n",
    "In practice, it's recommended to fine-tune a ViT model\n",
    "that was pre-trained using a large, high-resolution dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "image_classification_with_vision_transformer",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
